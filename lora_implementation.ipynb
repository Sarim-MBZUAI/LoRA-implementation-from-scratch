{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the model is deterministic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor() ,\n",
    "\n",
    "transforms.Normalize((0.1307,),(0.3081,))\n",
    "\n",
    "])\n",
    "\n",
    "# mnist_trainset = datasets.MNIST(root='./',train=True , download=True , transform= transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(mnist_trainset , batch_size = 10 , shuffle = True)\n",
    "\n",
    "# mnist_testset = datasets.MNIST(root='../data' , train = False , download = True , transform = transform)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(mnist_testset,batch_size = 10 , shuffle = True)\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigboy(nn.Module):\n",
    "\n",
    "    def __init__(self , hidden_size_1 = 1000 , hidden_size_2 =2000):\n",
    "\n",
    "        super(bigboy,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28 , hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1 , hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2,10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self , img):\n",
    "\n",
    "        x = img.view(-1 , 28*28) # what does this do??\n",
    "\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net = bigboy().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 595/595 [00:01<00:00, 427.03it/s, loss=0]\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader , net , epochs=5 , total_iterations_limit = None):\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "        data_iterator = tqdm(train_loader,desc=f'Epoch {epoch+1}')\n",
    "\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "\n",
    "            num_iterations+=1\n",
    "            total_iterations+=1\n",
    "            x,y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1,28*28))\n",
    "\n",
    "            loss = criteria(output,y)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "\n",
    "                return\n",
    "\n",
    "train(train_loader,net,epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear1.bias': tensor([-6.1826e-03, -1.1782e-02, -3.7209e-02,  3.5054e-02, -2.7781e-03,\n",
      "        -4.1314e-02,  7.9050e-03,  3.1383e-02, -1.0069e-02, -4.8849e-03,\n",
      "         3.2520e-02, -4.0118e-02, -1.1075e-02, -3.7919e-02,  1.5667e-02,\n",
      "         1.5197e-02, -8.3135e-03, -4.3067e-03,  2.0399e-02, -2.5367e-02,\n",
      "         2.6505e-02,  2.4834e-02,  4.6612e-03, -3.2186e-03,  1.2898e-02,\n",
      "         3.5037e-02, -5.2149e-03,  3.5380e-03, -1.2504e-02, -2.7773e-02,\n",
      "         4.1928e-03,  2.2047e-02, -3.1087e-02, -2.5898e-02, -1.6965e-02,\n",
      "         2.8190e-02,  1.6634e-02, -2.3039e-02, -7.7869e-03,  4.3765e-03,\n",
      "        -1.1344e-03, -4.4021e-03, -3.2640e-03,  1.1378e-02,  3.4757e-03,\n",
      "        -3.3046e-02, -3.0458e-02, -3.1046e-02, -2.7696e-02,  2.7731e-02,\n",
      "         1.5464e-02, -1.4977e-02,  2.2554e-03, -1.4871e-02,  2.2429e-02,\n",
      "         3.2983e-02, -2.1136e-02, -9.8758e-03, -2.7631e-03,  2.3001e-02,\n",
      "        -1.8290e-02, -2.8561e-02, -1.8739e-03, -1.5080e-02, -1.7763e-02,\n",
      "         1.5463e-02, -2.2214e-02,  2.0987e-02, -7.6257e-03,  1.1521e-02,\n",
      "         9.2751e-03,  5.1601e-03, -1.1495e-02,  8.9796e-03,  2.3394e-02,\n",
      "        -3.8265e-02,  2.5529e-02, -3.3696e-02, -2.4423e-02,  1.9773e-02,\n",
      "        -1.0040e-02,  2.2897e-02, -1.2940e-02,  5.8252e-03, -3.0227e-02,\n",
      "        -7.7797e-03, -2.8031e-02, -4.2007e-03, -2.5341e-02, -3.1566e-02,\n",
      "        -2.4197e-02,  8.7460e-03, -1.6006e-02,  2.2493e-02,  2.9670e-02,\n",
      "         9.4340e-03, -2.8151e-02, -2.4352e-03,  1.8025e-02, -3.7911e-02,\n",
      "         2.7632e-02, -2.2296e-02, -2.9592e-02,  2.5817e-03,  1.4688e-02,\n",
      "        -1.8732e-02, -1.2613e-02,  1.3941e-02, -3.7587e-02, -2.0024e-02,\n",
      "         4.6868e-04, -4.2979e-02, -1.0582e-02,  4.1725e-02, -2.7092e-02,\n",
      "         6.6997e-03, -2.3127e-02,  2.4636e-02, -9.8626e-03, -6.5750e-03,\n",
      "        -2.3991e-03,  1.9097e-02,  1.4974e-02, -1.6088e-02,  2.4793e-02,\n",
      "        -3.6992e-02,  3.3617e-02, -9.6816e-03, -3.2764e-02, -2.3594e-02,\n",
      "        -1.5793e-02,  3.9028e-02, -2.1962e-02,  2.3919e-02,  3.4903e-02,\n",
      "        -2.3381e-02, -1.6389e-02, -1.6212e-02, -1.9496e-02,  2.0082e-03,\n",
      "         3.4993e-03, -8.2568e-03, -1.6171e-04, -6.6597e-03, -2.5847e-02,\n",
      "         2.6364e-02,  2.4758e-02, -3.6048e-02,  2.2324e-02,  2.5393e-02,\n",
      "        -1.8321e-02, -2.3307e-02, -1.3428e-02, -2.0205e-02,  2.2228e-02,\n",
      "         4.1870e-02, -2.7670e-02,  2.4025e-03,  3.0356e-02,  3.9886e-02,\n",
      "        -3.9909e-02, -6.3029e-03,  1.8601e-02,  1.6080e-02, -1.0687e-02,\n",
      "        -1.8673e-02,  3.3446e-02,  2.8011e-02, -1.3828e-02,  3.8221e-03,\n",
      "         2.0593e-03, -2.2635e-03,  3.2115e-02,  2.1777e-02, -1.1134e-02,\n",
      "        -2.6849e-02, -6.0302e-03,  2.0191e-02,  6.7055e-03, -1.4536e-02,\n",
      "        -3.1828e-02, -1.9069e-02,  1.5288e-02,  1.3189e-02,  3.6539e-02,\n",
      "         1.3211e-02, -1.5248e-02, -2.3523e-02,  9.5604e-03,  8.6864e-03,\n",
      "        -2.4266e-03, -3.9584e-02, -5.9324e-04,  1.2957e-02,  6.2859e-03,\n",
      "        -2.2102e-02,  4.2402e-02,  1.6678e-02, -2.5533e-02, -4.2623e-03,\n",
      "         2.7799e-02, -1.6620e-02, -2.1664e-02,  6.6479e-03,  1.4324e-02,\n",
      "         7.6674e-03,  2.4401e-02,  2.9653e-02,  2.1662e-03, -3.6305e-03,\n",
      "        -2.0246e-02,  1.3041e-02,  2.9276e-02,  2.8804e-02,  3.4515e-02,\n",
      "        -2.5280e-02,  3.3266e-03, -2.3265e-02,  1.6234e-02,  2.4821e-02,\n",
      "        -2.8030e-02, -2.6258e-02,  7.6305e-03, -1.7707e-02,  1.4916e-02,\n",
      "         1.1104e-02, -3.6251e-03, -7.2380e-03, -2.4859e-02,  2.8758e-02,\n",
      "        -4.7825e-03,  2.8870e-02, -1.9338e-02,  2.8079e-02,  1.5332e-02,\n",
      "        -1.9629e-02, -3.1103e-02, -4.0691e-02, -1.4313e-02, -1.6734e-03,\n",
      "        -8.5020e-04,  6.0501e-03,  6.4404e-03, -3.3724e-02,  3.1088e-02,\n",
      "        -1.2128e-02,  9.4820e-03,  1.1308e-02, -5.6986e-03, -1.7268e-02,\n",
      "        -7.0159e-03, -3.8836e-02, -2.3416e-02,  3.1845e-02,  1.5840e-02,\n",
      "         3.1426e-03, -1.8257e-02,  4.8797e-03,  1.7468e-02, -1.4358e-02,\n",
      "         1.9365e-02, -3.6612e-02, -2.5320e-02,  1.8187e-02, -1.7041e-02,\n",
      "        -1.5169e-02,  3.0372e-02, -6.3077e-03, -5.2288e-03,  3.7549e-04,\n",
      "        -1.3480e-02, -7.7060e-03, -1.6864e-03, -2.6481e-02, -9.5350e-03,\n",
      "         1.5582e-02,  1.0864e-02, -2.5118e-02,  2.6238e-02, -1.7367e-02,\n",
      "        -1.7827e-02,  2.8891e-02, -1.2174e-03, -2.4149e-02, -1.8051e-02,\n",
      "         1.9063e-02,  6.5521e-04,  7.1986e-03, -3.0559e-02,  7.1724e-03,\n",
      "        -1.2727e-02, -1.3802e-02,  4.2306e-02,  1.3423e-02,  3.0778e-02,\n",
      "        -1.2040e-02,  6.7972e-04, -2.8702e-02, -1.7675e-02,  1.9003e-03,\n",
      "         5.1181e-03, -2.7449e-03,  3.7435e-02,  2.3813e-02,  9.5425e-03,\n",
      "         2.3028e-02,  1.3847e-02,  2.6492e-03,  3.8478e-02, -2.8227e-02,\n",
      "        -1.3777e-03,  1.0663e-02,  1.4513e-02, -7.3054e-03,  3.2704e-02,\n",
      "        -3.9301e-02, -1.7293e-04,  3.2665e-02,  3.1990e-02,  1.4204e-03,\n",
      "        -3.1805e-02, -4.1141e-02, -1.0403e-02,  2.5676e-02,  2.6064e-02,\n",
      "         2.7888e-02, -1.6237e-02,  2.6992e-03, -1.4706e-03, -1.9037e-02,\n",
      "         1.2929e-02,  9.5033e-03,  1.6747e-02,  3.7770e-02, -7.3186e-03,\n",
      "        -4.0641e-03, -1.0830e-02, -2.5785e-02, -3.9987e-02,  4.5939e-03,\n",
      "         2.6220e-02,  4.3524e-03, -2.4261e-02,  1.9893e-02,  2.6033e-02,\n",
      "         3.2958e-02,  2.8913e-02, -1.4467e-02, -1.1076e-02,  8.5488e-03,\n",
      "         1.4112e-02,  2.8004e-02, -1.6459e-02, -2.7797e-02, -1.1508e-03,\n",
      "         3.3521e-02, -6.0618e-03,  2.8256e-02,  1.2694e-02,  1.5717e-02,\n",
      "         8.2579e-04, -1.9933e-02,  1.3777e-02,  1.9484e-02, -1.5200e-02,\n",
      "        -1.8602e-02,  1.1986e-03, -3.7510e-02,  8.0516e-03,  2.4450e-02,\n",
      "        -1.7466e-02, -5.1919e-03,  3.7571e-02,  1.5524e-02, -2.0458e-02,\n",
      "         1.6753e-02, -2.1909e-02,  3.8017e-02,  3.6749e-02, -1.7879e-02,\n",
      "         4.0903e-02,  1.6679e-02, -6.0049e-03,  6.1180e-03, -6.9094e-03,\n",
      "         1.5257e-02, -2.1250e-02, -2.0446e-02, -3.8957e-02, -1.4670e-02,\n",
      "         4.5741e-03,  2.7608e-02, -1.1958e-02,  1.6797e-02, -1.4038e-02,\n",
      "         1.5102e-02, -3.4576e-02, -2.5475e-02, -3.8125e-02, -1.5772e-02,\n",
      "        -2.6334e-02,  6.1198e-03,  4.3848e-02, -6.9830e-03,  2.1770e-02,\n",
      "         3.8132e-02, -4.0338e-03,  2.2289e-02, -2.2823e-02,  5.1125e-03,\n",
      "        -5.7638e-05, -1.9015e-02, -3.8731e-02, -6.6111e-03, -1.8969e-02,\n",
      "        -1.8869e-02, -3.3263e-03,  5.3878e-03,  1.2320e-02,  2.1677e-02,\n",
      "         1.3509e-02,  3.3871e-03,  2.0578e-02,  3.3495e-03, -1.5319e-02,\n",
      "        -4.2651e-03,  3.3093e-02, -4.0422e-02,  1.9453e-02, -1.7239e-02,\n",
      "         1.5717e-03, -3.8169e-02, -1.2640e-02,  1.0702e-02,  3.8711e-02,\n",
      "         3.8635e-02,  2.8550e-02, -1.5300e-02,  4.6921e-03,  2.5131e-02,\n",
      "        -2.6154e-02,  2.3190e-02, -2.0292e-02,  4.2630e-02,  5.8210e-03,\n",
      "        -2.3446e-02,  4.0813e-03,  1.1509e-02, -2.2568e-02, -2.3200e-02,\n",
      "         2.0781e-02,  2.7508e-02, -7.1712e-04, -1.1042e-02,  9.6213e-03,\n",
      "        -2.1356e-02, -3.5492e-06, -1.4096e-02, -3.1880e-02, -2.2497e-02,\n",
      "        -1.7998e-02, -1.6554e-02,  2.6893e-02, -2.5544e-02,  2.7481e-02,\n",
      "         3.9843e-03,  7.6926e-03, -3.8918e-02, -2.3368e-02,  2.3242e-02,\n",
      "         8.8796e-03,  1.1641e-02, -2.8330e-02,  2.0817e-02,  2.9577e-02,\n",
      "         3.1292e-03,  4.1353e-02,  3.5604e-02,  4.0875e-03, -2.2181e-02,\n",
      "        -2.0055e-02, -3.2497e-02,  5.2945e-04,  2.7407e-02, -1.8514e-02,\n",
      "         1.7755e-02,  4.7006e-03, -2.8022e-02, -1.1373e-02, -3.3103e-03,\n",
      "         2.9502e-02,  1.8347e-02, -3.7312e-02, -3.9434e-02, -2.2033e-02,\n",
      "         3.1907e-02, -1.1848e-02, -2.7896e-02, -2.5319e-02,  1.4006e-03,\n",
      "        -2.2774e-02, -3.6061e-02,  7.7532e-03,  2.8389e-02,  1.5911e-02,\n",
      "         1.1891e-03, -1.7720e-02,  4.2632e-02,  2.0359e-02,  2.9311e-02,\n",
      "        -3.2595e-02,  2.3094e-02, -1.0250e-03,  3.4937e-03,  2.2894e-02,\n",
      "        -1.4896e-02,  1.6588e-02,  3.3225e-02,  4.4452e-03,  4.5697e-03,\n",
      "         2.3173e-02, -3.5343e-02, -2.1593e-02, -2.5219e-02, -1.3235e-02,\n",
      "         1.9779e-02,  2.0804e-02, -3.9434e-03,  5.5737e-03, -2.3719e-02,\n",
      "         3.2729e-03, -3.8558e-02,  1.0640e-02, -2.7902e-03, -2.7772e-02,\n",
      "        -2.5731e-02,  1.3362e-02, -2.2345e-02, -3.0523e-02,  1.9259e-02,\n",
      "         1.7950e-03, -2.7963e-02,  2.6034e-02, -1.5127e-02,  1.8023e-02,\n",
      "        -2.6058e-02,  2.9566e-02,  5.5245e-04,  3.4707e-02,  2.9472e-02,\n",
      "         2.2594e-02,  1.2975e-02,  2.9283e-02,  3.8805e-02, -1.2164e-02,\n",
      "        -1.2194e-02,  2.2440e-02, -1.9113e-02, -1.4608e-02, -1.1923e-02,\n",
      "        -6.8271e-03, -3.7369e-02, -1.3489e-02, -2.7961e-02,  2.5766e-02,\n",
      "         1.3731e-02, -4.9304e-03, -1.8464e-02,  1.4623e-02, -9.1793e-03,\n",
      "        -3.0233e-02,  2.5939e-02,  1.7430e-02, -1.7022e-02,  1.9405e-02,\n",
      "        -6.6030e-03,  2.8108e-02, -3.1168e-02,  1.8462e-02,  2.2164e-02,\n",
      "        -5.6985e-03, -2.8075e-03,  2.9523e-02, -2.6548e-02, -1.0933e-02,\n",
      "        -5.3823e-04,  8.4506e-03,  2.8319e-02, -1.3520e-02,  5.2482e-03,\n",
      "         2.0025e-02,  1.3514e-02, -9.0807e-03, -7.0477e-03,  3.0683e-02,\n",
      "         3.2690e-02, -3.1010e-03,  7.6557e-03,  3.6062e-03,  9.1789e-03,\n",
      "         3.2087e-02,  1.1262e-02,  2.4577e-02, -9.7628e-03, -6.6454e-03,\n",
      "        -1.2692e-02,  5.6492e-03,  1.8124e-02,  7.2021e-03,  2.0754e-02,\n",
      "         2.1490e-02, -1.8193e-02,  4.2680e-02, -1.6375e-02,  2.5126e-02,\n",
      "         1.8768e-03,  1.1568e-02,  2.9173e-02,  7.5870e-03,  2.8662e-02,\n",
      "        -1.1161e-02,  3.7222e-02,  5.2595e-03, -1.7802e-02,  1.3186e-02,\n",
      "        -4.1040e-02,  2.2116e-02,  3.6284e-03,  1.4913e-02,  3.7201e-03,\n",
      "         9.1985e-03, -4.0272e-02,  5.6944e-03,  2.6497e-02, -2.5421e-02,\n",
      "         1.0953e-02,  5.8887e-03, -1.4994e-03,  5.5209e-03, -1.9009e-02,\n",
      "         2.5734e-03, -3.3562e-02,  2.6223e-02,  3.8627e-02, -2.8345e-02,\n",
      "         3.4468e-02,  1.1581e-02,  1.1607e-02, -1.8583e-02, -2.5006e-02,\n",
      "         3.6802e-02, -3.6195e-03, -1.3638e-02,  1.0262e-02,  1.0788e-02,\n",
      "        -1.6818e-02, -2.6483e-03, -1.6872e-02,  1.7402e-02, -3.4717e-02,\n",
      "         1.2600e-02,  2.2525e-03,  7.8837e-03,  6.0350e-03, -3.0927e-02,\n",
      "         5.9851e-03, -1.9637e-02,  3.1701e-02,  1.7999e-02, -3.8759e-03,\n",
      "         3.8465e-02, -1.7519e-02,  2.5788e-02, -3.1453e-02, -1.1280e-02,\n",
      "        -1.8585e-02,  1.5795e-02, -2.3425e-02, -9.2510e-03,  1.0434e-02,\n",
      "         1.6602e-03, -1.0238e-02,  2.9489e-02,  3.4863e-02, -1.3955e-02,\n",
      "        -1.7452e-03,  3.6160e-02, -1.1740e-03, -1.0734e-02,  6.9383e-03,\n",
      "         4.1649e-04,  1.3355e-03, -4.9395e-03, -4.8007e-03, -1.7822e-02,\n",
      "        -3.4504e-02, -2.7781e-02,  2.7516e-02, -6.0733e-03, -3.0366e-02,\n",
      "         6.0850e-03,  2.5127e-02, -1.8710e-02, -2.6230e-02,  8.4786e-03,\n",
      "        -2.3197e-02,  5.6043e-03,  1.1404e-02,  1.1653e-02,  4.1354e-02,\n",
      "        -1.2946e-02, -1.5049e-04,  7.2536e-03,  1.9100e-02, -9.1576e-04,\n",
      "         1.1985e-04,  1.3588e-02, -2.4731e-02, -2.7256e-02,  2.6194e-02,\n",
      "        -2.1743e-02,  2.2158e-02,  2.3498e-02,  2.4916e-02, -9.6734e-03,\n",
      "         9.5092e-04, -2.2974e-02, -1.4523e-02,  7.0005e-03,  3.7542e-02,\n",
      "         7.9269e-03, -1.5913e-02,  1.7828e-02, -3.9478e-02,  2.1328e-02,\n",
      "        -2.5112e-02, -2.7346e-02, -1.6295e-02, -3.6261e-02,  1.3292e-02,\n",
      "         2.6796e-02,  8.7545e-03, -1.3525e-02, -1.1378e-03, -7.7119e-03,\n",
      "         1.3541e-03, -7.2530e-03, -1.4624e-03, -2.6893e-02, -6.6042e-03,\n",
      "        -2.1927e-02,  6.4057e-03, -1.2461e-02,  1.5926e-02,  2.3281e-03,\n",
      "         7.2177e-03,  5.2818e-03, -3.8216e-02, -2.4431e-03, -3.9295e-02,\n",
      "        -9.9856e-03,  2.7052e-02,  2.9629e-02,  4.0016e-02,  5.6027e-03,\n",
      "         2.4588e-02,  2.7814e-02, -5.5991e-03,  7.1622e-03,  4.1094e-03,\n",
      "        -7.6741e-03, -1.1988e-02,  3.0612e-03,  1.3431e-02, -4.4539e-03,\n",
      "        -1.1411e-02, -2.3282e-02, -6.4216e-03, -2.1816e-02, -1.6672e-02,\n",
      "        -6.3107e-03,  4.2729e-02, -2.7599e-02,  2.6704e-02, -2.3660e-02,\n",
      "         6.3849e-03,  2.0053e-02,  1.0759e-02,  1.1594e-02, -6.9884e-03,\n",
      "         2.6910e-02,  3.7845e-02,  6.7016e-03,  1.3671e-02, -1.6127e-02,\n",
      "         2.5545e-02,  4.1526e-02,  3.8120e-03, -2.4178e-03, -5.3561e-03,\n",
      "         3.8184e-03,  2.6475e-02, -9.1777e-03, -2.4158e-02, -1.4081e-02,\n",
      "        -1.1514e-02, -4.4949e-03,  2.1474e-02, -2.2532e-02, -1.8663e-02,\n",
      "        -2.5952e-02,  5.2184e-05,  1.7136e-02,  7.9688e-03,  2.6926e-02,\n",
      "        -4.0746e-02, -7.0272e-04,  1.7915e-02,  2.8550e-02,  3.9051e-02,\n",
      "         3.8949e-03,  1.5509e-02,  3.9273e-02,  7.7729e-03, -3.8614e-03,\n",
      "         2.4044e-02,  3.9520e-02,  5.8023e-03,  2.2506e-02, -7.0976e-03,\n",
      "        -1.7517e-02,  1.6072e-02, -2.4388e-02,  1.9445e-02,  9.5888e-04,\n",
      "        -1.1392e-02,  2.9431e-03,  5.9789e-03,  3.2951e-03, -8.1347e-03,\n",
      "         1.2820e-02,  1.2243e-02,  2.3883e-03,  4.1015e-02, -2.1721e-03,\n",
      "         8.3345e-04,  2.5582e-03, -1.7114e-02, -1.5110e-02,  1.8033e-02,\n",
      "        -6.7973e-03,  2.7505e-02,  1.5900e-02,  2.3317e-03,  2.2357e-02,\n",
      "         3.8656e-02,  1.6218e-02,  1.0199e-02, -1.8665e-02, -8.6906e-03,\n",
      "         2.1165e-02,  1.5399e-02,  3.0900e-02, -2.6838e-02,  1.5559e-02,\n",
      "         7.1014e-03, -5.6893e-03, -1.6130e-03, -3.1510e-02, -2.9668e-02,\n",
      "         2.0091e-03,  1.7908e-02,  3.2636e-02, -1.4167e-02,  3.0012e-02,\n",
      "        -2.2212e-02, -1.5911e-02,  3.3174e-02,  2.9145e-02,  2.5505e-02,\n",
      "        -4.0753e-02,  2.8225e-02,  2.2437e-02, -4.1231e-02, -9.8048e-04,\n",
      "        -1.5252e-02, -2.0012e-02,  6.0743e-03,  3.7778e-02, -1.7918e-03,\n",
      "         3.4757e-03,  1.8540e-02,  1.4545e-02, -1.3955e-02, -2.1494e-02,\n",
      "         2.0836e-02,  2.7062e-02, -2.4939e-02, -1.5988e-02,  3.8084e-02,\n",
      "        -1.6507e-02, -1.4139e-02,  1.4192e-03,  3.4023e-02,  9.4864e-03,\n",
      "        -3.0586e-02,  4.1106e-02,  2.6034e-02,  4.3898e-02,  1.0172e-03,\n",
      "         3.0510e-02, -7.5211e-03, -3.8911e-02,  1.7613e-02,  1.5218e-02,\n",
      "        -2.7404e-03,  2.8854e-02, -3.6364e-02,  1.6302e-02, -1.6687e-02,\n",
      "        -2.0112e-02, -2.4129e-02,  3.9929e-02,  2.1157e-02, -1.3312e-02,\n",
      "        -1.6647e-02,  3.6395e-02,  3.3339e-02,  1.0846e-02,  3.5227e-02,\n",
      "         1.8320e-02,  1.0209e-03, -1.5911e-02,  2.6107e-02,  7.7210e-03,\n",
      "        -3.5908e-02,  2.8148e-02, -1.0178e-02, -3.6552e-04, -2.1274e-02,\n",
      "         5.7277e-03, -7.7146e-03,  3.9020e-03,  1.7509e-03, -6.8373e-04,\n",
      "         3.3821e-02,  2.7058e-02, -1.3808e-02,  4.2149e-02,  2.9611e-02,\n",
      "         3.8894e-02, -4.0310e-02,  6.4134e-03,  2.4297e-02, -2.6448e-04,\n",
      "        -1.8895e-02, -1.0148e-02,  1.2291e-02, -2.1173e-02,  1.9938e-02,\n",
      "         4.0917e-02, -7.2806e-04,  1.6604e-02, -2.2361e-02,  9.7061e-03,\n",
      "        -2.0223e-02, -3.2356e-02, -2.0831e-02, -2.2512e-02,  2.7639e-03,\n",
      "        -4.0370e-02, -3.5874e-02, -3.6760e-02,  3.8959e-03,  3.2231e-04,\n",
      "         1.1389e-02, -4.0734e-02, -1.3605e-02, -4.0926e-02,  1.9625e-02,\n",
      "        -5.5863e-03, -2.1914e-03,  2.6219e-02, -2.0535e-02,  9.8728e-03,\n",
      "         4.3136e-02, -8.0008e-03,  3.8821e-02,  1.1003e-02,  7.8719e-03,\n",
      "         3.2636e-02, -3.4561e-03,  4.2528e-02,  1.4469e-02,  2.7810e-03,\n",
      "        -1.1429e-02,  2.3817e-02, -3.1667e-02,  1.7376e-02,  2.7944e-02],\n",
      "       device='cuda:0'), 'linear1.parametrizations.weight.original': tensor([[-0.0054, -0.0282, -0.0103,  ...,  0.0319,  0.0188, -0.0235],\n",
      "        [-0.0141, -0.0077,  0.0168,  ..., -0.0113,  0.0201,  0.0010],\n",
      "        [-0.0241, -0.0253,  0.0302,  ..., -0.0128,  0.0157, -0.0097],\n",
      "        ...,\n",
      "        [ 0.0114,  0.0026, -0.0227,  ..., -0.0282,  0.0089,  0.0389],\n",
      "        [-0.0093,  0.0149,  0.0234,  ...,  0.0004,  0.0237, -0.0374],\n",
      "        [-0.0088, -0.0381,  0.0278,  ..., -0.0057, -0.0319, -0.0368]],\n",
      "       device='cuda:0'), 'linear1.parametrizations.weight.0.lora_A': tensor([[-7.6867e-01,  1.5754e+00, -6.9803e-01,  1.5876e+00,  3.7192e-01,\n",
      "         -2.8010e-01, -1.3279e+00,  1.2715e-01, -1.9716e-01,  9.8228e-01,\n",
      "         -6.7151e-01, -1.9259e+00,  1.8006e+00, -2.0223e-02, -7.8210e-01,\n",
      "          1.2916e+00, -9.3502e-01,  1.7825e+00, -6.7430e-01, -1.9435e+00,\n",
      "         -2.1896e-01,  5.4124e-02, -1.5387e+00, -5.4652e-01, -2.1605e+00,\n",
      "          1.1957e+00,  1.6538e+00, -6.7432e-01,  2.2472e-01, -9.3252e-02,\n",
      "          1.1605e+00, -3.8935e-01,  1.7481e+00,  7.6590e-01,  2.3440e-01,\n",
      "          1.1179e-02, -5.5859e-02, -9.3045e-01, -4.9024e-01, -6.5587e-01,\n",
      "         -3.8251e-01, -5.5483e-01,  9.1054e-01, -2.2724e-01,  1.6871e+00,\n",
      "          2.8396e-02, -2.4173e-01, -4.8085e-01, -3.5577e-01, -2.0954e+00,\n",
      "         -4.4325e-01, -1.2620e-01, -8.1589e-01,  7.9303e-01, -4.5053e-01,\n",
      "         -9.5213e-01, -1.1753e+00,  2.4285e-01,  1.3851e-01, -2.3703e-01,\n",
      "         -9.9398e-01,  3.4641e-01,  1.4767e-01,  1.2583e-01, -5.1702e-01,\n",
      "          2.5393e-01,  4.0003e-01, -5.4010e-01, -7.8853e-01,  1.4666e-01,\n",
      "         -4.0963e-01,  2.8196e-01,  1.4446e+00,  4.4447e-02, -3.6677e-01,\n",
      "          2.2495e+00, -4.5369e-01, -6.5244e-01,  1.8278e+00, -2.4475e-01,\n",
      "          3.0723e-01, -5.1362e-02,  1.0639e-01,  8.4468e-01,  1.6482e+00,\n",
      "         -9.0471e-01, -3.5368e-01,  5.1092e-01,  7.4846e-02, -3.1132e-01,\n",
      "          2.7495e-01,  1.5943e+00,  1.3669e+00,  9.8733e-02,  4.3291e-02,\n",
      "         -2.0741e-01,  6.8286e-02, -6.0101e-01,  8.3034e-02,  2.6487e-01,\n",
      "         -6.5926e-01, -2.1682e-01, -8.6552e-02, -6.8353e-01, -9.6473e-01,\n",
      "         -2.5909e+00, -8.1758e-01, -4.6202e-01, -1.8413e+00, -7.1257e-01,\n",
      "         -5.2100e-02, -5.8294e-01,  1.1142e+00,  1.9029e-01,  1.0883e+00,\n",
      "          5.1456e-03,  1.0424e+00,  1.3634e+00, -2.7367e-01, -4.6555e-01,\n",
      "         -2.7614e-02,  1.3452e+00,  7.8942e-01, -1.5991e-02,  4.0168e-01,\n",
      "          7.2651e-01,  5.9869e-01,  1.3371e-01, -3.7933e-01, -8.3640e-01,\n",
      "          3.3555e-02,  1.4663e+00, -2.4307e-02,  9.0154e-01, -6.1865e-01,\n",
      "          8.1201e-01,  2.0634e-01, -7.5973e-01, -7.0243e-01, -4.2579e-01,\n",
      "          1.6563e-01,  1.7451e-03, -1.4927e-01,  8.6014e-01, -7.0359e-01,\n",
      "         -9.1455e-02,  1.4577e+00, -1.0663e+00,  1.2779e+00, -3.0144e-01,\n",
      "          3.4251e-01,  8.6141e-01, -9.0514e-01,  3.4082e-01, -9.5299e-01,\n",
      "         -4.7131e-01,  1.4664e+00, -6.3243e-01,  5.9067e-01,  1.1204e+00,\n",
      "         -4.3437e-01,  2.1881e-01,  4.6592e-01,  7.9147e-01, -1.7612e+00,\n",
      "         -1.1742e+00,  1.2899e+00, -1.3871e+00,  5.9903e-01,  2.6487e-01,\n",
      "          1.3399e-01,  3.0920e-02, -5.1533e-01, -2.3934e+00, -1.4008e+00,\n",
      "          4.9100e-01, -4.9095e-02,  1.2245e+00, -5.7226e-01, -7.6670e-01,\n",
      "          1.0530e+00,  5.1945e-01,  1.1921e+00,  1.0702e+00,  1.7506e+00,\n",
      "         -1.1355e+00,  1.8770e+00, -6.2120e-01, -1.0466e+00,  5.8145e-01,\n",
      "         -5.8290e-01, -8.2976e-01,  3.0715e-01, -5.7725e-01, -1.3614e+00,\n",
      "         -2.6653e+00,  5.5351e-01,  1.4376e+00, -6.0194e-01,  8.8050e-01,\n",
      "          1.5701e-01,  1.1033e+00, -2.1801e+00,  6.3723e-01,  1.3999e+00,\n",
      "         -1.4558e+00,  3.7529e-01, -1.7888e-01,  4.6475e-01,  5.6374e-01,\n",
      "          4.3414e-02, -5.6161e-01,  8.9119e-02,  4.2684e-01, -1.7087e+00,\n",
      "         -1.0687e+00, -1.2553e-01, -2.8189e-01,  3.5339e-01, -1.1306e-01,\n",
      "         -1.5803e-01,  2.3493e+00,  5.3127e-01,  2.1516e+00, -2.2257e+00,\n",
      "         -4.7402e-01, -7.9767e-01,  9.0383e-01,  1.3987e+00, -1.1758e+00,\n",
      "         -2.2818e-01, -2.6247e+00, -1.3051e+00, -1.1466e+00,  3.2948e-01,\n",
      "          2.0350e-02, -1.1831e+00, -3.3471e-01, -5.5102e-01, -1.7523e-01,\n",
      "         -2.0116e-01,  8.5180e-01,  3.0650e-01, -9.9206e-02,  2.2408e-01,\n",
      "         -6.8223e-01, -4.7241e-01,  4.4030e-01, -1.7468e+00,  2.4445e-02,\n",
      "          6.6143e-01, -4.8530e-02, -2.9332e-02,  9.3500e-02, -1.3690e+00,\n",
      "          1.3253e+00,  1.9776e+00,  2.8122e-01, -1.1326e+00,  6.0816e-01,\n",
      "          7.4023e-01,  7.7317e-01,  1.4194e+00, -3.4913e-01,  1.6875e+00,\n",
      "         -4.2823e-01, -4.6763e-01, -1.1327e+00, -3.1593e-01, -1.6046e+00,\n",
      "          9.9786e-01,  1.1339e+00,  1.9833e+00, -7.5514e-01, -2.1773e+00,\n",
      "          4.8599e-01, -4.8555e-01, -3.2136e-01, -8.9011e-03, -3.0848e-01,\n",
      "         -3.4634e+00, -9.7737e-01,  2.4590e+00,  1.0123e+00,  8.4043e-01,\n",
      "          1.6141e-01, -2.3209e+00,  4.0725e-01, -1.5684e-01, -1.9516e+00,\n",
      "         -8.9950e-01, -1.1394e+00,  3.7639e-01,  2.2975e-03,  2.8874e-02,\n",
      "         -8.7496e-01, -4.1714e-01, -6.4724e-01,  5.1541e-01, -7.5036e-01,\n",
      "         -6.8216e-01, -8.6671e-01, -8.0128e-01,  7.4096e-01, -1.2954e+00,\n",
      "          1.6559e+00,  5.7703e-01,  7.3233e-02,  1.5453e-01,  1.3802e+00,\n",
      "         -4.0701e-01,  4.6551e-01, -6.7273e-02,  1.3181e+00, -8.8701e-02,\n",
      "          9.6757e-01, -2.0007e-01, -5.4883e-01, -4.4999e-01,  7.8331e-01,\n",
      "          1.0947e+00,  5.6626e-01,  1.1687e+00, -2.5764e-01,  3.7706e-01,\n",
      "         -1.2703e-01,  1.3059e+00, -3.5311e-02, -2.3393e-01, -9.6165e-03,\n",
      "          1.1623e+00, -1.9265e+00, -1.0939e+00, -8.1193e-01, -9.7835e-02,\n",
      "          4.5742e-01,  8.0908e-01, -7.0920e-01,  1.0575e+00,  1.5098e+00,\n",
      "         -4.5239e-01, -8.0448e-02, -1.0274e-01,  5.7111e-01, -7.5731e-01,\n",
      "         -8.2254e-01, -1.1561e+00, -7.1822e-01,  1.5571e-01, -3.9500e-01,\n",
      "         -4.6561e-01,  9.0490e-01, -5.8848e-01,  1.9440e+00,  4.2437e-01,\n",
      "         -4.9201e-01,  2.0577e+00, -1.4026e+00, -6.8093e-01,  9.7041e-01,\n",
      "         -1.4716e-02, -8.3315e-01,  9.8710e-01, -7.6382e-01, -1.5226e-01,\n",
      "          1.3414e+00, -4.1053e-01,  1.1602e+00,  2.6405e-01, -1.9304e-01,\n",
      "         -1.4385e+00, -5.2027e-01, -1.2198e+00,  8.1895e-01,  3.0056e-01,\n",
      "         -4.5832e-01, -3.6147e-01,  1.1413e-01, -5.7162e-01, -3.4909e-01,\n",
      "          7.6186e-02,  9.0847e-01, -1.1291e-01, -6.1376e-01, -1.3887e+00,\n",
      "         -1.3503e+00,  1.3724e+00, -6.4595e-01,  1.8631e+00, -4.3448e-01,\n",
      "          2.1505e+00,  1.7694e+00, -4.7647e-01, -1.6380e-01, -9.8195e-01,\n",
      "          1.2402e+00,  1.4219e+00,  9.6700e-01,  1.2552e+00, -1.7576e+00,\n",
      "         -2.5640e-01,  9.4017e-01, -1.2489e+00,  8.8735e-01,  2.2796e-01,\n",
      "         -2.4388e+00,  1.0617e+00,  6.5868e-01,  2.3923e+00, -5.0784e-01,\n",
      "          9.3009e-01,  1.9471e+00, -5.7975e-01,  1.3791e+00,  7.7780e-01,\n",
      "         -3.8479e-01, -2.8739e-01,  7.6465e-01, -1.1274e+00,  1.5753e-01,\n",
      "          7.9605e-02, -1.9234e+00, -6.8487e-01,  4.6515e-01, -6.9417e-01,\n",
      "         -7.9949e-01,  5.5281e-01,  6.3809e-01,  7.0526e-01, -1.5250e+00,\n",
      "          1.7610e+00, -9.3021e-01,  9.8256e-01,  2.6913e+00, -2.7959e-02,\n",
      "          1.5240e+00, -1.1483e-01,  6.8503e-01, -3.6695e-02, -1.3472e+00,\n",
      "          1.1055e+00, -5.6956e-01,  1.1631e+00,  1.1199e+00,  1.2614e+00,\n",
      "          3.1034e-01, -1.1440e+00,  8.4815e-02,  1.4791e+00,  7.0376e-01,\n",
      "         -9.4631e-01, -9.7765e-01,  9.7117e-01,  4.5890e-01, -3.7595e-01,\n",
      "         -2.3955e+00, -2.0095e-01,  2.7497e-01,  7.7068e-01,  1.3756e+00,\n",
      "          2.4660e-01, -2.4308e+00,  4.2562e-01,  6.7124e-01,  1.9065e-01,\n",
      "          1.1918e+00, -1.4290e+00, -6.9117e-01,  1.1759e+00,  4.0402e-01,\n",
      "         -7.3274e-01, -7.2581e-01,  3.9710e-01,  2.1001e-01, -4.7751e-01,\n",
      "         -9.6402e-01,  1.7922e-01,  6.1558e-01,  1.9401e-01,  1.6845e-01,\n",
      "          7.4000e-01, -1.0544e-01,  1.4780e-01, -1.6448e+00, -1.7460e+00,\n",
      "         -5.4503e-01, -1.4943e+00,  2.5000e-01, -1.6422e+00,  1.8828e+00,\n",
      "          1.5718e+00, -1.3897e+00, -6.7567e-01,  6.3343e-01,  9.7750e-01,\n",
      "         -8.1163e-01, -3.8522e-01, -1.1230e+00,  1.0195e-01, -1.5607e+00,\n",
      "         -1.7593e+00, -5.8939e-01,  8.3765e-02, -1.8036e-01,  7.7204e-01,\n",
      "         -1.0641e+00,  5.5163e-01, -2.3068e-01,  1.6434e+00, -4.6558e-01,\n",
      "          1.1483e-01, -8.1048e-01, -1.2544e+00,  1.3423e-01, -3.8178e-01,\n",
      "         -1.4199e+00, -4.4298e-01, -5.5920e-01, -4.3228e-01,  2.5946e-01,\n",
      "          1.4113e+00, -1.5855e-02,  1.2139e-01,  6.8437e-02,  5.9828e-01,\n",
      "         -9.4429e-01,  4.0836e-01, -7.1961e-01, -4.7295e-02, -3.0536e-01,\n",
      "          9.7052e-01, -9.7181e-01, -1.2873e+00, -6.3494e-01, -9.9092e-03,\n",
      "          1.3606e+00, -2.7124e-01,  8.0467e-01,  5.9936e-01,  5.3010e-01,\n",
      "         -1.0458e-01,  1.1166e+00,  1.0190e+00,  1.5894e+00,  1.3278e+00,\n",
      "          2.3942e-01, -6.2013e-01, -3.4066e-01, -1.4143e-01,  1.8067e+00,\n",
      "         -1.5403e+00, -6.4853e-01,  7.4795e-01, -6.9289e-01,  1.2549e+00,\n",
      "          1.4559e+00,  9.6218e-01, -1.1710e+00,  1.8217e-01, -1.3855e+00,\n",
      "          9.0469e-01, -6.8600e-01, -1.0441e-01,  1.1131e+00, -1.5573e+00,\n",
      "         -1.5352e-01, -1.3333e+00, -1.0106e+00, -7.9977e-01,  6.3425e-01,\n",
      "          1.6804e-01,  1.0031e+00,  7.0456e-01, -8.5026e-01, -4.0734e-01,\n",
      "          5.1779e-01,  4.9554e-01, -1.0494e+00,  8.8786e-01, -2.1623e-01,\n",
      "         -1.0267e+00, -3.2953e-01,  6.4334e-01, -6.3428e-01,  2.5834e+00,\n",
      "          8.3902e-01,  2.5855e+00,  4.4730e-01,  6.7298e-01,  4.8618e-01,\n",
      "         -1.6868e+00,  9.0673e-01, -8.6406e-01,  1.0624e+00,  1.4767e-01,\n",
      "         -1.0490e+00,  6.9664e-01, -4.5375e-01, -7.4036e-01,  2.7816e+00,\n",
      "         -1.4506e+00,  2.7641e-01,  2.2904e-01, -8.5824e-01, -8.8279e-01,\n",
      "          1.3517e+00,  1.2523e-01, -1.1136e+00, -5.4215e-01,  4.2908e-01,\n",
      "         -6.5522e-02, -9.9849e-01, -4.0528e-01, -5.5159e-02,  1.3824e+00,\n",
      "         -5.8562e-01, -8.3539e-01,  9.3620e-01,  1.7352e+00,  8.1409e-02,\n",
      "         -5.0580e-02, -4.5311e-01,  1.1820e-01,  6.5275e-01, -4.9086e-01,\n",
      "          1.0794e+00,  8.9774e-01,  1.2764e-01, -6.3265e-01, -2.6157e-01,\n",
      "          4.8578e-01, -3.2228e-01, -8.1279e-01,  1.4007e+00,  6.9147e-01,\n",
      "          4.5762e-01,  3.1166e-01,  1.0968e+00,  1.0219e+00,  4.8410e-01,\n",
      "          2.9049e-01, -7.6121e-01, -1.3196e-01, -3.2411e-01,  3.8851e-03,\n",
      "         -1.0665e+00, -1.1713e-01, -1.6247e+00, -1.0297e+00,  2.7669e-01,\n",
      "          8.2791e-01,  3.0209e-01, -2.7172e+00,  2.1920e+00,  3.1615e-01,\n",
      "          1.2460e+00,  3.9169e-01,  9.2699e-01, -1.2665e+00, -1.0379e+00,\n",
      "         -4.3491e-01, -7.4571e-01,  1.1176e+00,  1.7874e+00, -1.6145e+00,\n",
      "         -6.4400e-01,  9.2930e-03, -8.8756e-02,  1.9913e-01,  8.6246e-01,\n",
      "         -4.7598e-01, -1.1563e+00,  6.3005e-01, -3.5121e-01, -1.1283e+00,\n",
      "          1.5974e-01, -5.5477e-01,  3.2574e-01, -1.1665e+00,  5.0279e-01,\n",
      "         -1.4739e-01,  1.4840e+00, -6.9739e-01,  2.1504e+00,  4.1292e-01,\n",
      "          1.3549e+00, -2.7531e-01, -2.5974e-01,  9.3860e-01, -4.0125e-01,\n",
      "          1.4425e+00,  3.3557e-01,  5.6000e-02,  2.2764e-01, -8.8973e-01,\n",
      "         -5.5952e-01,  8.7894e-01,  1.3159e+00,  7.6126e-01, -9.5861e-01,\n",
      "         -2.1288e+00,  1.9446e+00,  5.8095e-01, -1.0731e+00,  1.3656e+00,\n",
      "         -4.6271e-01,  6.2185e-01, -3.2536e-01, -3.2179e+00, -1.4397e+00,\n",
      "          9.7284e-01, -3.5276e-01, -9.7599e-01,  5.4193e-01, -5.3833e-01,\n",
      "         -8.0581e-01,  5.7251e-01, -3.2835e-01, -7.6835e-01, -1.0953e+00,\n",
      "          5.7435e-01,  1.6135e+00,  1.3136e+00, -1.9890e+00, -7.0204e-02,\n",
      "          1.3701e+00,  2.3397e-01, -8.6725e-01,  2.9587e-01, -7.7066e-01,\n",
      "          4.3540e-01, -1.4580e+00,  1.1995e+00, -4.6237e-01,  5.6692e-01,\n",
      "         -3.6817e-01,  1.8965e-01,  1.9586e-01, -3.4344e-01,  4.3504e-01,\n",
      "         -8.1674e-01,  6.3549e-01,  4.9129e-01, -1.1413e+00, -1.1840e+00,\n",
      "          4.9921e-01, -1.2944e+00,  1.5644e+00,  4.0907e-02,  7.2362e-01,\n",
      "          9.6724e-01, -5.8176e-01,  6.7360e-01, -1.7615e+00,  5.2143e-01,\n",
      "          5.8010e-01, -2.7348e-02, -1.3807e+00, -6.5851e-01, -1.5511e+00,\n",
      "         -6.3500e-01, -2.1744e-01,  1.0378e+00,  1.0554e+00,  1.3881e-01,\n",
      "          1.1369e+00,  2.8833e+00, -7.9019e-01, -4.9864e-01,  8.9074e-02,\n",
      "          7.6813e-01,  5.0054e-01, -1.3782e+00,  8.4342e-01,  1.0197e+00,\n",
      "          3.2517e-01, -5.2944e-01, -1.3159e-01, -1.0650e-02, -6.5625e-01,\n",
      "         -2.7342e-01,  1.3262e+00, -1.2909e+00, -1.4967e+00]], device='cuda:0'), 'linear1.parametrizations.weight.0.lora_B': tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0'), 'linear1.parametrizations.weight.1.lora_A': tensor([[ 8.9269e-01, -4.7203e-01, -4.2309e-01,  1.2770e+00,  2.5712e-01,\n",
      "         -1.3667e+00,  1.5526e+00,  4.4134e-01, -3.3151e-02, -5.9427e-02,\n",
      "          1.5357e+00, -2.2548e-01,  1.4723e+00,  5.9154e-01, -3.8612e-01,\n",
      "         -2.1981e+00,  8.9570e-02, -2.5667e-01, -1.8733e+00,  9.1718e-01,\n",
      "          3.9214e-01,  1.5522e-02, -9.4908e-01,  5.6708e-01, -2.2041e-01,\n",
      "          1.2624e+00, -4.3767e-01, -2.2838e+00, -2.6478e-01, -6.5998e-01,\n",
      "          2.3526e+00,  1.9921e+00,  8.6562e-01, -8.5401e-01, -1.0150e+00,\n",
      "          8.9922e-01, -1.0406e+00,  1.3789e+00, -7.5070e-02, -2.5420e+00,\n",
      "         -8.8356e-01, -4.2863e-01,  9.8200e-01, -7.2761e-02,  2.2147e+00,\n",
      "          6.5830e-01,  1.7012e-01, -1.7276e+00, -6.7226e-01, -1.3733e+00,\n",
      "          3.1809e-01,  4.2229e-01,  2.6070e-01, -5.4761e-01,  5.4612e-01,\n",
      "         -1.5576e-01,  6.7945e-01,  2.8608e+00, -3.0087e-01, -5.0485e-01,\n",
      "          1.5484e+00,  3.5371e-01, -3.8763e-01, -1.5958e+00, -1.7013e-01,\n",
      "         -2.8975e-03,  2.7326e-01, -3.8358e-01, -1.0821e+00, -8.9423e-01,\n",
      "         -1.0491e+00, -4.4714e-02,  4.9296e-02,  2.2027e-01,  2.7279e-01,\n",
      "         -8.5289e-01, -4.8918e-01,  5.1348e-01,  9.7726e-01,  3.1063e-01,\n",
      "         -5.7721e-01, -4.7904e-01,  8.3893e-01,  8.7268e-01, -5.1019e-01,\n",
      "          1.0180e-01, -2.9969e-01, -1.1795e+00, -1.5557e+00,  6.6850e-01,\n",
      "          9.3949e-01,  1.1812e-01, -3.7662e-01,  7.3555e-01, -2.1418e-01,\n",
      "         -1.9868e+00, -9.3130e-01,  1.2689e+00,  1.4279e+00, -7.5777e-01,\n",
      "         -1.3245e+00,  3.7548e-01,  1.3640e+00, -1.7080e+00,  9.7673e-01,\n",
      "         -3.7646e-02, -1.7790e+00, -1.9663e-01,  1.6368e+00,  6.9020e-01,\n",
      "          9.4206e-01, -1.8824e+00,  4.3169e-01,  2.0385e-01,  1.3062e+00,\n",
      "         -1.2638e-01,  1.4077e+00,  1.1883e+00,  4.3275e-01, -2.2972e+00,\n",
      "         -4.7542e-01,  1.5175e+00, -8.2403e-01,  1.2877e+00, -2.8251e-02,\n",
      "          1.9188e+00,  3.5239e-01,  6.9319e-01, -1.2324e+00,  3.0549e-01,\n",
      "         -1.6413e-01, -7.6153e-01, -1.5321e+00, -4.1875e-02,  5.0481e-01,\n",
      "         -2.9843e-01,  5.2811e-01, -8.1009e-02, -6.7442e-01,  3.2943e-01,\n",
      "         -1.2809e+00,  1.1614e+00, -1.0382e+00, -1.3526e+00, -7.1136e-01,\n",
      "         -1.5042e-02, -8.9668e-01, -1.9591e+00, -5.4805e-01, -8.4899e-01,\n",
      "         -4.6432e-01, -6.2697e-01,  8.2549e-01, -2.2583e-01,  2.1158e+00,\n",
      "         -8.7073e-01, -1.0106e+00, -6.2679e-01, -8.6070e-01, -8.1184e-01,\n",
      "          1.9903e+00,  2.5820e-01, -3.9223e-01,  8.8159e-02,  6.6455e-01,\n",
      "          6.0466e-01, -7.1397e-01,  7.1419e-01,  2.3460e+00, -1.9587e-01,\n",
      "          2.5882e-02, -4.1446e-02,  5.3328e-01, -9.2438e-01,  4.4236e-01,\n",
      "         -9.6381e-02, -2.9797e-01, -7.0162e-01, -1.6267e-01,  1.6037e-01,\n",
      "         -2.8411e-01, -1.6027e-01,  1.5723e+00, -1.6794e+00,  2.1382e+00,\n",
      "          1.3452e-01, -1.2703e+00, -1.2346e+00, -1.2701e+00, -1.3192e+00,\n",
      "         -8.9017e-02,  3.5158e-01, -2.8645e-01,  1.1573e+00,  9.9869e-01,\n",
      "         -1.6534e+00, -2.9551e-02, -5.5528e-01,  1.0703e+00, -4.5510e-01,\n",
      "          7.4937e-01, -6.5475e-01, -3.5193e-01, -5.7896e-01,  5.1404e-02,\n",
      "          9.4175e-01,  6.9624e-01, -5.7724e-01, -7.7623e-01,  1.0941e+00,\n",
      "          1.8941e-01,  1.0264e+00, -1.3370e+00,  5.5577e-01, -1.7213e-01,\n",
      "         -3.4731e-01,  4.2610e-01,  4.7862e-01, -2.0094e-02, -5.7554e-01,\n",
      "          2.7607e-01,  4.2638e-01, -2.6058e-01, -7.0862e-01,  7.9110e-01,\n",
      "         -1.6259e+00, -3.4465e-01,  6.4938e-01, -1.0581e+00,  1.2200e+00,\n",
      "          2.4481e-01,  1.9558e+00, -1.0463e-01,  2.9844e-01,  1.9242e+00,\n",
      "         -1.2271e+00,  5.5928e-01, -2.7782e-02, -4.8517e-01, -5.5038e-01,\n",
      "          4.3864e-01,  9.0010e-01, -2.0316e+00,  1.1903e+00,  1.1814e+00,\n",
      "          2.2345e+00,  1.1232e-01,  5.6606e-01, -1.4611e+00,  6.3620e-02,\n",
      "         -3.0527e-01, -1.1247e+00, -6.8127e-01,  9.6882e-01,  1.1919e+00,\n",
      "         -5.7529e-01,  1.6621e+00, -1.1719e+00,  2.5820e-01,  8.3295e-02,\n",
      "          6.8836e-01,  7.2702e-01, -8.9424e-01, -4.7694e-01, -5.5725e-01,\n",
      "          1.2980e-01,  1.0061e+00,  4.9494e-01,  1.8380e+00,  3.0396e-01,\n",
      "         -9.5540e-01,  9.4518e-01,  5.6964e-01, -2.3842e-01,  1.8494e+00,\n",
      "         -3.4773e-01,  5.0750e-01, -1.5519e+00,  6.7312e-01, -1.7418e+00,\n",
      "          1.9406e+00,  3.3029e+00,  4.1099e-01,  5.9201e-01, -1.7496e-01,\n",
      "          6.0849e-01,  6.3723e-01, -7.8542e-01, -4.5455e-02,  1.3273e+00,\n",
      "          7.1667e-01, -1.2318e+00,  3.0327e-01, -1.0345e+00, -1.7861e-02,\n",
      "          1.3106e+00, -1.5999e-01, -2.3515e-01, -3.1557e-01, -6.3565e-02,\n",
      "         -6.8163e-01,  1.0594e+00,  1.0608e-01,  3.4144e-01, -1.7604e+00,\n",
      "          2.3081e-01,  2.1507e-01,  1.3984e+00, -2.4136e-01,  1.7301e-01,\n",
      "         -1.9750e+00, -6.5338e-01,  1.4255e+00, -1.0625e+00, -4.8792e-01,\n",
      "         -3.7185e-01, -2.2369e-01,  9.2176e-02,  2.0230e-01,  1.2391e+00,\n",
      "         -1.3187e+00, -1.0610e+00,  1.0701e+00, -3.4038e-01, -1.8017e-02,\n",
      "         -3.4210e-01,  3.0252e+00,  2.5693e-01,  2.2812e+00,  1.4048e+00,\n",
      "          6.4863e-01,  3.1119e-01,  1.3026e+00, -1.2777e+00, -8.9269e-01,\n",
      "         -2.0441e-01, -3.3311e-02, -4.3671e-02,  9.9526e-02, -7.8711e-01,\n",
      "          8.5469e-01, -4.3190e-01, -3.7617e-01, -6.1722e-01,  3.6073e-01,\n",
      "         -7.2465e-01,  1.7191e+00,  6.2674e-02, -5.3449e-01,  4.8811e-01,\n",
      "          5.6887e-01,  5.7170e-01, -1.0801e-01, -4.0804e-01,  1.5538e+00,\n",
      "          9.6045e-01, -7.8579e-01,  3.7381e-01, -8.7007e-02, -7.5663e-01,\n",
      "         -4.7017e-01,  1.0223e+00, -8.2204e-01,  1.2805e+00, -1.2692e+00,\n",
      "          1.6710e-01, -9.5827e-01,  7.2939e-01, -5.7121e-01, -4.9966e-01,\n",
      "         -1.2868e+00,  3.2168e-01,  3.0575e-01,  9.4754e-01,  1.2757e+00,\n",
      "         -7.7716e-01, -1.2842e+00,  5.3024e-01,  2.7766e-01, -2.3896e-01,\n",
      "          9.6918e-01,  3.2038e-01,  1.6355e-01,  3.5992e-01,  2.3976e-01,\n",
      "          9.6072e-01,  7.5176e-01,  2.1784e-01, -9.6548e-01, -3.8682e-01,\n",
      "         -6.8713e-01,  8.8621e-01, -1.0632e+00,  2.2919e+00,  6.8420e-01,\n",
      "          4.6938e-01,  5.1401e-01, -1.3691e+00,  2.4046e+00, -6.2899e-01,\n",
      "         -1.0063e-02, -8.1171e-01, -1.0959e+00, -2.6171e-01,  2.6269e-01,\n",
      "         -1.0481e+00, -1.7346e-01,  6.5110e-01,  8.1930e-01,  5.8418e-01,\n",
      "         -1.0634e+00,  1.3471e+00, -1.5559e+00, -1.0329e-01, -8.4116e-01,\n",
      "         -1.6644e-01,  6.9144e-01, -6.2222e-01, -5.3795e-01, -3.9777e-01,\n",
      "          6.4534e-01,  1.6829e+00, -7.5316e-01, -7.7161e-01,  1.6584e+00,\n",
      "         -2.7094e-02,  3.0128e-01, -8.6268e-01, -4.1285e-01,  8.8536e-03,\n",
      "         -9.5012e-02,  4.3304e-01, -1.1372e+00, -2.0481e+00, -1.4896e-01,\n",
      "          1.2119e+00,  1.7551e+00,  6.6834e-01,  9.1451e-01, -1.4258e+00,\n",
      "          1.3486e+00,  1.8941e+00, -9.2723e-01, -1.6702e-01, -8.5435e-01,\n",
      "          1.4673e+00,  1.8749e-01,  5.5120e-01,  5.7897e-01, -1.8070e+00,\n",
      "          7.8988e-01,  1.5772e+00,  8.7297e-01, -2.6307e-01,  3.1068e-01,\n",
      "         -8.7997e-02, -2.8387e+00, -7.7351e-01,  8.5868e-01, -6.2365e-01,\n",
      "         -1.3354e+00, -6.2172e-02, -7.7913e-01, -2.8039e-01,  9.8591e-02,\n",
      "          3.8038e-01, -6.9647e-01, -1.2104e-01, -1.3470e-01, -5.8735e-01,\n",
      "          1.7065e-01,  5.9646e-02, -8.6100e-01,  1.2771e+00, -5.4053e-01,\n",
      "         -3.1420e-01,  1.9532e+00, -3.8098e-01,  7.9762e-01,  5.5417e-01,\n",
      "         -1.5639e-02,  6.4170e-01, -7.8944e-01, -1.3804e+00,  1.3902e-01,\n",
      "         -4.7523e-01, -2.5956e+00,  1.8601e+00,  1.9911e-01,  1.0643e+00,\n",
      "          9.4400e-01,  1.2594e+00, -8.0111e-01,  1.5172e-01,  1.7474e-01,\n",
      "          1.3366e-01, -1.1737e+00, -6.0917e-02, -5.3598e-03,  7.2812e-01,\n",
      "          5.2647e-01, -8.6922e-01, -1.8087e+00,  2.2284e-01, -4.3914e-01,\n",
      "          9.3682e-01, -3.0581e+00,  9.2530e-01,  4.1567e-01, -6.3406e-01,\n",
      "         -1.2577e+00,  1.2009e-01, -5.5165e-02,  4.6463e-01,  1.2665e-01,\n",
      "          9.3655e-01, -8.5551e-01, -1.0662e-01, -9.2885e-01,  1.1242e+00,\n",
      "          3.8297e-01, -9.8386e-01,  8.8709e-02,  1.0492e-01, -2.5866e-01,\n",
      "          4.2381e-01,  1.8079e+00,  1.7201e+00, -1.4727e+00,  1.3839e-02,\n",
      "          9.8046e-01, -2.5769e-01,  1.1743e+00,  6.4337e-02,  1.1345e+00,\n",
      "          8.1360e-02,  1.2474e+00,  8.1757e-01, -4.4585e-01, -1.5592e-01,\n",
      "          4.0027e-01,  6.2436e-01,  8.0253e-01, -5.5203e-01, -1.6292e+00,\n",
      "          8.0981e-01, -2.3653e-01, -1.4305e+00,  3.7601e-01,  1.6485e+00,\n",
      "          7.4719e-01,  5.0397e-01, -8.7104e-01,  1.0797e+00, -1.5863e+00,\n",
      "          2.0976e-01,  6.3422e-01,  4.0178e-01, -2.0153e+00,  6.1010e-02,\n",
      "          2.2872e-01, -1.6350e-01, -9.7850e-02, -9.6530e-01,  1.0315e+00,\n",
      "         -8.5063e-01, -3.3678e-02,  1.3276e-02,  2.3900e+00,  6.2663e-02,\n",
      "          1.4711e-01, -8.8323e-01,  9.6546e-01,  1.5707e+00,  4.5689e-01,\n",
      "         -1.0353e-01, -6.4541e-01,  9.1104e-01, -3.8261e-02, -2.3993e+00,\n",
      "          2.8085e-02,  3.4456e-01, -1.0245e+00, -9.0014e-01,  2.3125e+00,\n",
      "         -9.1411e-01, -4.7732e-01, -2.1160e+00,  7.2722e-01,  1.7704e+00,\n",
      "          1.7725e+00, -7.6505e-01, -7.9172e-01, -1.6405e-01,  5.8716e-01,\n",
      "          5.0055e-01,  1.5987e+00, -3.0330e-02, -6.6782e-01, -5.3168e-02,\n",
      "          3.4167e-01,  1.3960e+00, -4.4620e-01, -7.8952e-01,  1.9583e-03,\n",
      "         -1.3646e-01,  3.5403e-01, -3.1996e-01,  3.8536e-01, -1.1536e+00,\n",
      "          4.2559e-02, -3.5223e-01, -5.8038e-02, -1.7189e-01,  7.7367e-01,\n",
      "         -2.8782e-01,  5.9541e-01, -6.6094e-01,  4.4185e-02,  2.0347e-01,\n",
      "         -5.1236e-01,  6.1553e-01,  6.2153e-01, -1.3290e+00, -9.8569e-01,\n",
      "         -1.1144e+00, -1.2799e-01,  4.6751e-01,  2.8364e-01,  2.7515e-01,\n",
      "         -3.7539e-01, -4.4886e-02, -2.2411e+00,  1.2333e+00,  3.0846e-01,\n",
      "         -9.5311e-01, -1.5029e+00,  1.2323e-01,  5.5282e-02, -8.1203e-01,\n",
      "          2.5792e+00,  8.0572e-01, -4.1469e-01, -1.3556e+00,  3.8992e-01,\n",
      "         -2.3910e+00, -6.4019e-01,  3.5403e-01,  1.0727e+00,  2.1584e+00,\n",
      "          1.3887e+00,  1.8033e+00,  8.6832e-01, -4.6049e-01, -1.9276e-01,\n",
      "          1.3134e+00, -3.5035e-01, -1.3244e+00, -5.1978e-01,  1.5624e-01,\n",
      "          1.1491e-01,  8.1596e-01, -9.0442e-01,  3.3751e-01, -1.1517e-01,\n",
      "         -1.4188e+00,  3.8673e-01,  7.2569e-01,  4.7558e-01, -3.8362e-01,\n",
      "          4.8835e-01, -7.3397e-01,  2.1256e+00,  5.2218e-01, -1.3586e+00,\n",
      "          1.5830e-01, -1.3979e+00,  3.9626e-01, -2.0186e+00, -2.6720e-01,\n",
      "         -4.5710e-01, -6.1522e-01,  6.3682e-01, -7.0522e-01,  7.1185e-01,\n",
      "          1.0059e+00,  1.3426e-01, -1.3278e+00, -2.0238e-01, -3.8763e-01,\n",
      "          1.0858e+00, -1.0764e+00, -1.2928e+00,  1.2894e+00,  1.9211e-01,\n",
      "         -1.9994e+00, -1.7782e+00,  7.7823e-01,  6.3848e-01, -3.2323e-01,\n",
      "         -4.3703e-01, -1.4200e+00, -8.9707e-01, -1.2049e+00,  4.7949e-01,\n",
      "          1.1396e+00, -1.8343e+00, -1.3820e-01, -2.9071e+00, -2.6011e+00,\n",
      "         -1.6487e+00,  3.6918e-01,  1.7851e+00, -1.2442e+00, -6.3388e-01,\n",
      "          3.1136e-01,  1.0006e-01, -1.2383e+00, -6.4398e-01, -1.3701e+00,\n",
      "          2.9246e-01, -1.2119e+00, -7.3893e-03, -4.3622e-03, -1.2242e+00,\n",
      "         -1.7768e+00, -3.0650e-01, -3.8287e-01, -1.0664e+00, -8.1170e-01,\n",
      "          1.0468e+00,  8.7470e-01,  5.4673e-01,  5.3413e-01,  8.3234e-02,\n",
      "         -6.5436e-01, -6.0609e-01, -1.1679e+00, -8.2118e-01, -9.5302e-01,\n",
      "         -5.6569e-01,  3.3791e-01, -7.9177e-01, -1.6740e-02, -5.9714e-01,\n",
      "         -3.6520e-01,  3.1692e-01, -5.4600e-01, -5.9233e-01,  2.6021e-01,\n",
      "          1.2639e-01,  5.9980e-02,  6.8348e-03, -1.2796e+00,  1.2862e+00,\n",
      "          8.0488e-01,  8.1582e-01, -9.8651e-01,  1.2813e+00,  6.6372e-02,\n",
      "          2.5163e-02,  1.2880e+00,  1.4462e-01,  6.1519e-01, -3.8647e-01,\n",
      "         -1.2223e+00,  1.3756e-01, -6.3384e-01, -1.5497e-01,  2.4527e-01,\n",
      "          9.1611e-01,  1.6368e+00,  3.3854e-01,  5.0011e-02, -1.7908e+00,\n",
      "          1.1272e+00,  1.4885e-01,  7.7292e-01, -1.9918e+00,  4.7530e-01,\n",
      "         -3.1826e-01, -1.9644e+00, -1.3885e-01, -6.6425e-01]], device='cuda:0'), 'linear1.parametrizations.weight.1.lora_B': tensor([[ 0.0000e+00],\n",
      "        [ 4.9411e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.4789e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.1271e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.5509e-23],\n",
      "        [ 5.2374e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.2880e-23],\n",
      "        [ 6.4109e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.2016e-23],\n",
      "        [ 7.3348e-23],\n",
      "        [ 7.3287e-23],\n",
      "        [ 6.9043e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.4987e-23],\n",
      "        [ 9.1791e-30],\n",
      "        [ 6.4170e-23],\n",
      "        [ 7.4077e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.7363e-23],\n",
      "        [ 5.0520e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.9464e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.3598e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.0128e-23],\n",
      "        [ 7.4146e-23],\n",
      "        [ 6.3394e-23],\n",
      "        [ 5.4889e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6732e-23],\n",
      "        [ 7.9219e-23],\n",
      "        [ 5.7728e-23],\n",
      "        [ 5.4671e-23],\n",
      "        [ 7.1719e-23],\n",
      "        [ 9.4859e-23],\n",
      "        [ 7.5481e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.7458e-23],\n",
      "        [ 6.6107e-23],\n",
      "        [ 4.5773e-23],\n",
      "        [ 6.6432e-23],\n",
      "        [ 6.4655e-23],\n",
      "        [ 6.7031e-23],\n",
      "        [ 7.9815e-23],\n",
      "        [ 6.0632e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.0229e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7950e-23],\n",
      "        [ 5.7071e-30],\n",
      "        [ 5.9173e-23],\n",
      "        [ 6.4731e-23],\n",
      "        [ 6.4751e-23],\n",
      "        [ 6.4406e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.4401e-23],\n",
      "        [ 3.4658e-23],\n",
      "        [ 6.6198e-23],\n",
      "        [ 6.1347e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.1109e-23],\n",
      "        [ 7.3132e-23],\n",
      "        [ 5.9615e-23],\n",
      "        [ 8.8166e-23],\n",
      "        [ 7.5573e-23],\n",
      "        [ 6.5734e-23],\n",
      "        [ 6.2182e-23],\n",
      "        [ 4.6767e-23],\n",
      "        [ 8.4841e-30],\n",
      "        [ 5.5147e-23],\n",
      "        [ 6.9966e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5605e-23],\n",
      "        [ 6.0166e-23],\n",
      "        [ 7.1221e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 1.0642e-29],\n",
      "        [ 6.7433e-23],\n",
      "        [ 5.4431e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.0980e-23],\n",
      "        [ 7.4151e-23],\n",
      "        [ 8.1237e-23],\n",
      "        [ 8.6439e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.0355e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.2851e-23],\n",
      "        [ 8.2473e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.3684e-30],\n",
      "        [-1.0836e-30],\n",
      "        [ 7.2263e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7885e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.1204e-23],\n",
      "        [ 8.0523e-30],\n",
      "        [ 7.3153e-23],\n",
      "        [ 8.7047e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.1318e-23],\n",
      "        [ 9.9553e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7608e-23],\n",
      "        [ 6.0848e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.2626e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.5088e-30],\n",
      "        [ 5.7535e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.4201e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4446e-23],\n",
      "        [ 7.0246e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.9638e-23],\n",
      "        [ 8.6673e-23],\n",
      "        [ 6.3599e-23],\n",
      "        [ 7.9947e-23],\n",
      "        [ 6.4578e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.7338e-23],\n",
      "        [ 6.4292e-23],\n",
      "        [ 7.8332e-23],\n",
      "        [ 7.3298e-30],\n",
      "        [ 6.0254e-23],\n",
      "        [ 8.2718e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 3.3278e-23],\n",
      "        [ 7.3911e-23],\n",
      "        [ 5.5875e-30],\n",
      "        [ 5.6275e-23],\n",
      "        [ 7.7326e-23],\n",
      "        [ 7.6982e-23],\n",
      "        [ 6.4008e-23],\n",
      "        [ 8.0584e-23],\n",
      "        [ 6.8274e-23],\n",
      "        [ 9.2899e-30],\n",
      "        [ 8.2779e-30],\n",
      "        [ 7.1370e-23],\n",
      "        [ 7.0448e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.5043e-23],\n",
      "        [ 7.1176e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.6303e-30],\n",
      "        [ 8.2865e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.6563e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.3070e-23],\n",
      "        [ 1.0098e-29],\n",
      "        [ 7.3032e-23],\n",
      "        [ 6.8292e-23],\n",
      "        [ 5.5130e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.0323e-30],\n",
      "        [ 5.7511e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0637e-23],\n",
      "        [ 7.1217e-23],\n",
      "        [ 6.1623e-23],\n",
      "        [ 6.5810e-23],\n",
      "        [ 7.6803e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.9460e-23],\n",
      "        [ 5.6410e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.5366e-23],\n",
      "        [ 7.2943e-23],\n",
      "        [ 2.1978e-23],\n",
      "        [ 7.8046e-23],\n",
      "        [ 5.8298e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.2172e-23],\n",
      "        [ 7.4789e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.8253e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0751e-30],\n",
      "        [ 6.0163e-23],\n",
      "        [ 9.2036e-23],\n",
      "        [ 7.6259e-30],\n",
      "        [ 6.8384e-23],\n",
      "        [ 6.5031e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.3318e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.2066e-23],\n",
      "        [ 6.2287e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.8890e-23],\n",
      "        [ 4.8703e-23],\n",
      "        [ 5.0035e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.3628e-23],\n",
      "        [ 6.3506e-23],\n",
      "        [ 5.3518e-30],\n",
      "        [ 7.1200e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.1818e-30],\n",
      "        [ 8.5692e-30],\n",
      "        [ 7.3965e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.3374e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.1993e-23],\n",
      "        [ 8.4924e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.6662e-23],\n",
      "        [ 7.9952e-30],\n",
      "        [ 8.8052e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.5806e-23],\n",
      "        [ 9.1509e-23],\n",
      "        [ 7.4813e-23],\n",
      "        [ 8.1053e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.8407e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4580e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.4812e-23],\n",
      "        [ 9.2201e-30],\n",
      "        [ 7.7604e-30],\n",
      "        [ 7.1615e-23],\n",
      "        [ 6.7155e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.5939e-30],\n",
      "        [ 7.4481e-23],\n",
      "        [ 3.4774e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.2944e-23],\n",
      "        [ 8.0051e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.0144e-23],\n",
      "        [ 1.0214e-29],\n",
      "        [ 6.3164e-23],\n",
      "        [ 6.8061e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.4325e-23],\n",
      "        [ 8.8496e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4063e-30],\n",
      "        [ 4.4902e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.4229e-23],\n",
      "        [ 8.5068e-30],\n",
      "        [ 3.1204e-23],\n",
      "        [ 7.8393e-23],\n",
      "        [ 7.2753e-23],\n",
      "        [ 7.5283e-23],\n",
      "        [ 7.5599e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5390e-23],\n",
      "        [ 6.5640e-23],\n",
      "        [ 7.9077e-23],\n",
      "        [ 8.2706e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.3163e-23],\n",
      "        [ 4.5166e-23],\n",
      "        [ 7.2422e-23],\n",
      "        [ 6.0109e-23],\n",
      "        [ 7.4253e-30],\n",
      "        [ 7.5126e-23],\n",
      "        [ 7.8421e-23],\n",
      "        [ 7.0599e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.5938e-30],\n",
      "        [ 5.7081e-30],\n",
      "        [ 6.9599e-23],\n",
      "        [ 8.0517e-30],\n",
      "        [ 5.8144e-23],\n",
      "        [ 9.4471e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.9148e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.5044e-23],\n",
      "        [ 7.1972e-23],\n",
      "        [ 7.2768e-23],\n",
      "        [ 6.8447e-23],\n",
      "        [ 6.1463e-23],\n",
      "        [ 7.2078e-30],\n",
      "        [ 7.3233e-23],\n",
      "        [ 6.4428e-23],\n",
      "        [ 6.9571e-23],\n",
      "        [ 3.1820e-24],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.2942e-23],\n",
      "        [ 9.9147e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.0303e-30],\n",
      "        [ 7.4368e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.0262e-23],\n",
      "        [ 8.8420e-30],\n",
      "        [ 6.4879e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.9983e-30],\n",
      "        [ 6.8656e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5027e-23],\n",
      "        [ 4.4242e-23],\n",
      "        [ 6.4220e-23],\n",
      "        [ 5.0830e-23],\n",
      "        [-1.9373e-30],\n",
      "        [ 7.4061e-30],\n",
      "        [ 9.8933e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6701e-23],\n",
      "        [ 7.2131e-23],\n",
      "        [ 7.8649e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7183e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5883e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.9768e-30],\n",
      "        [ 7.1299e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.6258e-30],\n",
      "        [ 5.9087e-23],\n",
      "        [ 3.2236e-23],\n",
      "        [ 8.1522e-23],\n",
      "        [ 9.3924e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7116e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.2441e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.9047e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.3016e-23],\n",
      "        [ 6.1379e-23],\n",
      "        [ 6.3952e-23],\n",
      "        [ 7.8239e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.2254e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.6621e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6554e-23],\n",
      "        [ 7.8859e-23],\n",
      "        [ 4.4721e-23],\n",
      "        [-5.8789e-31],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.5214e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [-1.2384e-24],\n",
      "        [ 5.6934e-32],\n",
      "        [ 5.3992e-30],\n",
      "        [ 6.5628e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4623e-23],\n",
      "        [ 7.3193e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.4467e-30],\n",
      "        [ 5.9542e-23],\n",
      "        [ 8.7147e-30],\n",
      "        [ 6.2075e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.7448e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 3.8372e-23],\n",
      "        [ 7.3259e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.9618e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.8416e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.4825e-30],\n",
      "        [ 7.3260e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.8017e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.8858e-23],\n",
      "        [ 6.7411e-23],\n",
      "        [ 7.1108e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.1806e-23],\n",
      "        [ 8.8961e-30],\n",
      "        [ 3.8590e-23],\n",
      "        [ 1.7350e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.0152e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.5810e-23],\n",
      "        [ 7.2074e-23],\n",
      "        [ 8.3692e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.7118e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5299e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.3258e-23],\n",
      "        [ 7.7451e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 1.0196e-29],\n",
      "        [ 6.8511e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.0758e-23],\n",
      "        [ 8.7832e-23],\n",
      "        [ 5.8241e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6729e-23],\n",
      "        [ 7.5411e-23],\n",
      "        [ 6.7160e-23],\n",
      "        [ 6.1561e-23],\n",
      "        [ 5.3035e-23],\n",
      "        [ 6.4810e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.5560e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.1545e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0217e-23],\n",
      "        [ 7.6265e-23],\n",
      "        [ 7.4455e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.3247e-23],\n",
      "        [ 6.4764e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.8162e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.0128e-23],\n",
      "        [ 8.0825e-23],\n",
      "        [ 6.8859e-23],\n",
      "        [ 7.2258e-30],\n",
      "        [ 7.8943e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.0857e-23],\n",
      "        [ 9.0626e-30],\n",
      "        [ 7.1542e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6996e-23],\n",
      "        [ 5.1967e-23],\n",
      "        [ 7.8464e-23],\n",
      "        [ 6.2225e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.3802e-23],\n",
      "        [ 3.7985e-23],\n",
      "        [ 7.9590e-23],\n",
      "        [ 4.9989e-23],\n",
      "        [ 5.1092e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.4769e-23],\n",
      "        [ 4.3080e-23],\n",
      "        [ 6.2361e-23],\n",
      "        [ 7.3124e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.1004e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.7368e-23],\n",
      "        [ 6.3900e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.0429e-23],\n",
      "        [ 7.7550e-23],\n",
      "        [ 7.9245e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.5365e-23],\n",
      "        [ 6.9727e-23],\n",
      "        [ 6.2964e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0647e-23],\n",
      "        [ 7.1081e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.2070e-23],\n",
      "        [ 6.4554e-23],\n",
      "        [ 7.2690e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4737e-23],\n",
      "        [ 8.7900e-30],\n",
      "        [ 4.9090e-23],\n",
      "        [ 8.6630e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.3878e-23],\n",
      "        [ 1.5386e-24],\n",
      "        [ 7.7309e-23],\n",
      "        [ 6.9022e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0952e-23],\n",
      "        [ 7.4540e-30],\n",
      "        [ 8.7278e-23],\n",
      "        [ 1.1035e-29],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.0369e-23],\n",
      "        [ 4.2269e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.4220e-23],\n",
      "        [ 8.3784e-23],\n",
      "        [ 8.2039e-23],\n",
      "        [ 7.7220e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.7084e-23],\n",
      "        [ 7.0360e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.8591e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.2058e-23],\n",
      "        [ 5.3225e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.1845e-23],\n",
      "        [ 8.4517e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.0111e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.8391e-23],\n",
      "        [ 6.6895e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6925e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.8241e-30],\n",
      "        [ 7.8572e-23],\n",
      "        [ 6.8889e-30],\n",
      "        [ 5.8417e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.9208e-23],\n",
      "        [ 7.2859e-23],\n",
      "        [ 8.0651e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.7357e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.7475e-30],\n",
      "        [ 6.0118e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.1800e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.0048e-23],\n",
      "        [ 8.0238e-23],\n",
      "        [ 7.8933e-23],\n",
      "        [ 4.2252e-23],\n",
      "        [ 7.1946e-23],\n",
      "        [ 9.3884e-30],\n",
      "        [ 6.5142e-30],\n",
      "        [ 6.5750e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4049e-23],\n",
      "        [ 3.6084e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.7890e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.9758e-23],\n",
      "        [ 6.4631e-23],\n",
      "        [ 4.4433e-23],\n",
      "        [ 6.7934e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 1.0042e-29],\n",
      "        [ 1.0037e-29],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.2857e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6224e-23],\n",
      "        [ 4.9977e-23],\n",
      "        [ 6.6328e-23],\n",
      "        [ 8.8941e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.5537e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.3295e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.5207e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4790e-23],\n",
      "        [ 6.3436e-23],\n",
      "        [ 7.7857e-23],\n",
      "        [ 6.2582e-23],\n",
      "        [ 5.1994e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6981e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.3800e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.7026e-30],\n",
      "        [ 5.4573e-23],\n",
      "        [ 5.4100e-23],\n",
      "        [ 7.9510e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5761e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.8611e-23],\n",
      "        [ 1.0648e-29],\n",
      "        [ 6.7473e-23],\n",
      "        [ 8.2509e-23],\n",
      "        [ 5.1630e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.7917e-30],\n",
      "        [ 6.1827e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.4808e-30],\n",
      "        [ 6.2822e-23],\n",
      "        [ 7.4492e-23],\n",
      "        [ 8.2234e-30],\n",
      "        [ 7.3126e-30],\n",
      "        [ 7.3894e-23],\n",
      "        [ 6.6464e-23],\n",
      "        [ 6.0105e-30],\n",
      "        [ 7.2978e-23],\n",
      "        [ 7.4803e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.3133e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.6997e-23],\n",
      "        [ 6.4308e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7765e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.5244e-23],\n",
      "        [ 9.3757e-30],\n",
      "        [ 9.9680e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.2696e-23],\n",
      "        [ 8.6140e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.8062e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0751e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.0558e-23],\n",
      "        [ 8.3733e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.6708e-23],\n",
      "        [ 5.9764e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.2085e-23],\n",
      "        [ 7.5784e-30],\n",
      "        [ 8.7780e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 3.1329e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.9009e-23],\n",
      "        [ 7.2112e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.8748e-23],\n",
      "        [ 7.2004e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4375e-30],\n",
      "        [ 7.7348e-23],\n",
      "        [ 6.7589e-23],\n",
      "        [ 5.9489e-23],\n",
      "        [ 6.3846e-23],\n",
      "        [ 4.9891e-23],\n",
      "        [ 8.1294e-30],\n",
      "        [ 8.7364e-23],\n",
      "        [ 9.3073e-30],\n",
      "        [ 6.7787e-23],\n",
      "        [ 5.5035e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0101e-23],\n",
      "        [ 7.8179e-30],\n",
      "        [ 6.5571e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.2717e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.6899e-30],\n",
      "        [ 5.9170e-23],\n",
      "        [ 6.4087e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5408e-30],\n",
      "        [ 6.0064e-23],\n",
      "        [-1.0866e-31],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7218e-23],\n",
      "        [ 5.7722e-23],\n",
      "        [ 4.4746e-23],\n",
      "        [ 5.2585e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 1.0049e-29],\n",
      "        [ 9.7839e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.9094e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.5669e-23],\n",
      "        [ 6.7091e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7551e-23],\n",
      "        [ 6.6797e-23],\n",
      "        [ 8.5855e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 3.8884e-23],\n",
      "        [ 1.9738e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.4908e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.3614e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.5685e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6248e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.9270e-30],\n",
      "        [ 9.3481e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.4172e-23],\n",
      "        [ 6.1638e-23],\n",
      "        [ 8.7749e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5100e-23],\n",
      "        [ 5.4172e-23],\n",
      "        [ 6.8933e-23],\n",
      "        [ 7.4643e-23],\n",
      "        [ 8.1943e-23],\n",
      "        [ 6.0185e-23],\n",
      "        [ 7.3045e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.8574e-23],\n",
      "        [ 8.2871e-30],\n",
      "        [ 5.9949e-23],\n",
      "        [ 7.4711e-23],\n",
      "        [ 8.3015e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.6379e-23],\n",
      "        [ 7.6827e-23],\n",
      "        [ 7.6582e-23],\n",
      "        [ 8.0091e-23],\n",
      "        [ 4.7765e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.0298e-30],\n",
      "        [ 7.1529e-23],\n",
      "        [ 5.8142e-23],\n",
      "        [ 9.1977e-23],\n",
      "        [ 5.8237e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5342e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.3345e-30],\n",
      "        [ 8.4328e-23],\n",
      "        [ 7.5507e-23],\n",
      "        [ 4.8148e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.7961e-23],\n",
      "        [ 7.8840e-23],\n",
      "        [ 7.0136e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.2543e-23],\n",
      "        [ 3.8092e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.8280e-23],\n",
      "        [ 6.8171e-23],\n",
      "        [ 5.0223e-23],\n",
      "        [ 4.4199e-23],\n",
      "        [ 7.5174e-23],\n",
      "        [ 6.4006e-23],\n",
      "        [ 8.0543e-23],\n",
      "        [ 5.8193e-23],\n",
      "        [ 6.9661e-23],\n",
      "        [ 6.8371e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.2050e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.1488e-23],\n",
      "        [ 4.9618e-23],\n",
      "        [ 6.2517e-23],\n",
      "        [ 8.5951e-23],\n",
      "        [ 8.6478e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4302e-23],\n",
      "        [ 5.0167e-23],\n",
      "        [ 6.8326e-30],\n",
      "        [ 8.8667e-23],\n",
      "        [ 7.2314e-23],\n",
      "        [ 8.6134e-30],\n",
      "        [ 7.5603e-23],\n",
      "        [ 7.9720e-23],\n",
      "        [ 1.0462e-29],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.1589e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 4.8191e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4518e-23],\n",
      "        [ 7.4147e-23],\n",
      "        [ 7.1416e-30],\n",
      "        [ 9.3003e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.8497e-23],\n",
      "        [ 6.7460e-23],\n",
      "        [ 7.0500e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.6118e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 3.2474e-23],\n",
      "        [ 6.4606e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.9730e-23],\n",
      "        [ 6.1058e-23],\n",
      "        [ 6.5901e-23],\n",
      "        [ 6.2952e-23],\n",
      "        [ 7.8651e-30],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.4117e-30],\n",
      "        [ 8.5672e-23],\n",
      "        [ 4.7466e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0663e-23],\n",
      "        [ 6.8730e-23],\n",
      "        [ 7.4445e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.2066e-30],\n",
      "        [ 6.5978e-23],\n",
      "        [ 5.7826e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7462e-23],\n",
      "        [ 6.5482e-23],\n",
      "        [ 7.6768e-23],\n",
      "        [ 9.0208e-30],\n",
      "        [ 8.1467e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.2083e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.2773e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4797e-23],\n",
      "        [ 6.3636e-23],\n",
      "        [ 5.6563e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.9938e-23],\n",
      "        [ 9.4757e-30],\n",
      "        [ 6.8473e-23],\n",
      "        [ 8.4456e-30],\n",
      "        [ 8.0900e-30],\n",
      "        [ 5.2512e-23],\n",
      "        [ 7.6207e-23],\n",
      "        [ 7.7314e-23],\n",
      "        [ 7.3001e-23],\n",
      "        [ 8.7907e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.7148e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.7120e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.0756e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 6.6354e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 8.2057e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.4729e-23],\n",
      "        [ 7.6457e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.5660e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 9.1518e-30],\n",
      "        [ 8.1996e-23],\n",
      "        [ 7.6783e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.2351e-23],\n",
      "        [ 1.5065e-23],\n",
      "        [ 7.3877e-23],\n",
      "        [ 7.1792e-23],\n",
      "        [ 6.4330e-23],\n",
      "        [ 7.3122e-23],\n",
      "        [ 7.6492e-30],\n",
      "        [ 7.4265e-23],\n",
      "        [ 6.9903e-23],\n",
      "        [ 7.0231e-24],\n",
      "        [ 5.8550e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.4630e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.7500e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.8733e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.4867e-23],\n",
      "        [ 5.5147e-23],\n",
      "        [ 9.3667e-30],\n",
      "        [ 6.3910e-23],\n",
      "        [ 7.3142e-23],\n",
      "        [ 6.1770e-23],\n",
      "        [ 6.4162e-23],\n",
      "        [ 1.9585e-23],\n",
      "        [ 9.1808e-30],\n",
      "        [ 7.8080e-23],\n",
      "        [ 6.7422e-23],\n",
      "        [ 8.1874e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 0.0000e+00],\n",
      "        [ 7.4402e-23],\n",
      "        [ 0.0000e+00],\n",
      "        [ 5.5911e-23],\n",
      "        [ 5.8065e-23],\n",
      "        [ 6.4372e-23],\n",
      "        [ 3.6369e-23],\n",
      "        [ 6.7234e-23],\n",
      "        [ 8.3083e-23]], device='cuda:0'), 'linear2.bias': tensor([-0.0158,  0.0115, -0.0222,  ..., -0.0047, -0.0331,  0.0192],\n",
      "       device='cuda:0'), 'linear2.parametrizations.weight.original': tensor([[ 0.0042,  0.0329, -0.0011,  ..., -0.0128,  0.0267,  0.0169],\n",
      "        [ 0.0345, -0.0173, -0.0122,  ..., -0.0102, -0.0067,  0.0223],\n",
      "        [-0.0270,  0.0366,  0.0069,  ...,  0.0312, -0.0035, -0.0135],\n",
      "        ...,\n",
      "        [-0.0234, -0.0166,  0.0221,  ...,  0.0109, -0.0160, -0.0254],\n",
      "        [ 0.0141, -0.0343, -0.0313,  ..., -0.0054,  0.0063, -0.0093],\n",
      "        [-0.0151, -0.0247,  0.0017,  ...,  0.0251,  0.0199,  0.0199]],\n",
      "       device='cuda:0'), 'linear2.parametrizations.weight.0.lora_A': tensor([[ 2.4125e-01,  5.5666e-01, -8.3553e-01,  5.6460e-01, -1.3383e+00,\n",
      "         -2.7892e-01,  3.5733e-01, -1.7455e+00,  2.7718e-01,  1.0142e-01,\n",
      "         -1.8637e-02, -5.2619e-01, -1.1698e-02, -2.3494e+00,  2.0658e-01,\n",
      "          3.5768e-01,  1.1376e+00,  3.1651e-01, -1.4735e+00, -1.0998e+00,\n",
      "         -3.6314e-01,  3.9828e-01,  1.9390e-02,  2.1716e+00, -2.4444e-01,\n",
      "          6.9838e-02,  1.4216e+00,  1.6621e+00, -1.6642e+00,  5.9190e-01,\n",
      "          5.5128e-02,  2.5801e-01,  5.0634e-01,  1.0160e+00, -2.7274e-01,\n",
      "         -1.4527e+00, -7.4124e-01,  5.1712e-01, -1.2587e+00,  1.4556e+00,\n",
      "         -1.3316e-02,  6.8533e-01, -1.8235e+00,  1.3115e+00,  1.5151e-01,\n",
      "         -1.7137e+00,  1.7015e+00,  2.3847e-01, -4.9654e-01,  1.0453e+00,\n",
      "         -4.5865e-01,  3.2599e-01,  1.2190e+00, -5.4453e-01,  5.7321e-01,\n",
      "          5.3155e-01,  1.3332e+00, -5.8832e-01, -1.0529e+00, -7.9659e-01,\n",
      "          5.5919e-01, -1.8007e+00,  9.4993e-01,  5.9234e-01,  9.0131e-02,\n",
      "          2.3779e+00,  6.8429e-02, -1.0655e+00,  2.1660e+00,  3.8736e-01,\n",
      "         -3.3914e-01,  8.7759e-02,  3.1996e-01, -8.1842e-02,  3.6661e-01,\n",
      "         -7.1359e-01, -1.2547e-01, -2.1825e+00, -5.7751e-02,  2.7194e-01,\n",
      "         -7.4248e-01,  9.6601e-01, -2.5377e-01,  1.2707e+00,  7.7855e-01,\n",
      "         -2.2765e-01, -1.0149e+00,  1.4147e-01, -8.1704e-01, -3.4107e-01,\n",
      "         -2.4027e-01, -2.0527e-01,  4.6696e-01,  1.4721e-01, -5.4617e-01,\n",
      "         -1.0051e+00,  5.2753e-01, -1.0974e-01, -4.7658e-01,  5.5595e-01,\n",
      "         -1.9657e+00,  1.5403e-01,  2.5092e-01,  1.2073e+00, -1.0239e+00,\n",
      "         -1.5120e+00,  5.8042e-01,  8.6526e-01, -9.4297e-01,  1.2048e+00,\n",
      "         -3.7129e-02,  5.3074e-03,  1.2027e+00,  1.6501e+00,  8.3401e-01,\n",
      "          7.1085e-02, -9.6782e-01,  1.5218e-01,  5.6148e-01, -2.2734e+00,\n",
      "          4.3491e-01,  1.3700e+00, -2.6744e-01,  4.7235e-01, -5.5811e-01,\n",
      "         -1.8406e+00,  7.2806e-01,  2.2380e+00, -1.9294e-01,  8.2548e-01,\n",
      "          1.3623e+00,  5.8223e-01, -2.2860e-02,  9.8077e-01, -1.8554e+00,\n",
      "          6.1774e-01,  9.7184e-01, -1.2690e+00, -7.1630e-01, -9.8814e-01,\n",
      "         -2.3164e+00,  7.3365e-01, -4.9969e-02, -2.1955e-01, -8.0644e-01,\n",
      "          1.5794e-01,  2.7892e-02, -1.2410e+00, -1.6542e-01,  1.0679e+00,\n",
      "         -2.1680e+00, -2.7623e+00, -1.2615e+00, -1.9826e+00, -7.5265e-01,\n",
      "          5.6140e-01,  4.4513e-01,  6.4419e-02, -1.1162e+00, -9.1155e-01,\n",
      "         -3.3881e-01, -1.0552e+00,  4.1575e-01,  5.0548e-01,  1.3793e-01,\n",
      "         -4.8580e-01,  8.0603e-01,  6.4971e-01,  8.2482e-01, -4.7425e-01,\n",
      "          8.6944e-01, -1.1583e+00,  3.4101e-01,  3.3615e-01, -7.3359e-01,\n",
      "          1.6100e-01,  5.5271e-01, -1.6702e-01,  3.0751e-01,  1.2657e+00,\n",
      "         -1.9566e-02, -9.1074e-01,  1.3943e-01,  2.2476e-01, -1.6039e-01,\n",
      "         -3.7755e+00,  7.0760e-01,  4.0885e-01, -3.1370e-02, -8.9037e-01,\n",
      "         -4.0835e-02,  5.0883e-01, -6.9994e-01,  3.3379e-01, -8.2972e-01,\n",
      "          4.0982e-01, -1.4744e+00, -1.4477e+00,  5.0197e-01, -2.4545e-01,\n",
      "          1.5362e+00, -1.3248e+00, -5.5894e-02, -6.8962e-01, -6.4067e-01,\n",
      "          2.6245e-02,  4.7148e-01, -1.6028e+00,  3.2471e-01,  1.1984e+00,\n",
      "          1.8836e+00, -9.8851e-01,  5.7343e-01, -1.2535e+00,  2.3451e+00,\n",
      "          1.4332e+00,  1.4844e+00,  6.1435e-01, -1.1687e+00,  9.8211e-01,\n",
      "          8.8473e-01,  1.2224e-01, -8.1857e-01,  3.6493e-02, -1.3833e-01,\n",
      "          2.7569e-02,  1.1494e+00, -2.1725e-02, -9.2655e-01,  4.0555e-02,\n",
      "          6.9142e-01,  1.0037e+00, -8.4584e-01,  2.0933e+00,  3.5466e-01,\n",
      "          1.0542e+00, -2.3756e-01, -2.1659e+00,  3.8480e-01, -7.6000e-01,\n",
      "         -1.1555e+00, -1.0203e+00, -2.7883e-01,  1.3066e+00,  1.9073e+00,\n",
      "         -1.2985e+00, -1.4405e-01, -1.1278e+00, -1.0461e+00, -9.3312e-02,\n",
      "         -1.1098e-01, -8.3190e-01, -1.9258e+00,  1.9983e+00, -1.1021e+00,\n",
      "         -4.7311e-01,  4.1564e-01,  4.7917e-01, -1.9693e+00, -3.6775e-01,\n",
      "          1.0080e+00,  1.7251e-01,  1.3466e-02,  1.1947e-02, -4.3681e-01,\n",
      "         -4.0216e-01, -7.2101e-01,  8.1077e-01, -1.0153e-01, -7.2633e-01,\n",
      "          9.5734e-01,  1.1196e+00, -1.0344e+00,  8.7146e-01,  4.6385e-01,\n",
      "         -4.1103e-02, -7.9049e-01, -3.4527e-01, -8.3098e-02,  1.3168e+00,\n",
      "          1.2241e+00, -1.1582e+00,  4.7546e-01, -5.8530e-02,  3.9099e-01,\n",
      "          9.9839e-01, -8.4260e-01,  1.5887e+00, -1.3486e+00,  1.1127e+00,\n",
      "          1.8720e+00, -1.2335e+00, -4.9801e-01,  5.0356e-01,  5.7660e-01,\n",
      "         -1.5316e-01, -5.2877e-01, -1.7330e+00, -8.1929e-01, -6.9888e-01,\n",
      "         -8.4592e-01, -1.0741e-01,  3.2613e-01, -3.2031e-01,  3.9135e-01,\n",
      "          5.4531e-01, -4.3635e-01, -3.5689e-02,  4.5083e-01, -4.2662e-01,\n",
      "         -9.0106e-01, -2.7566e-01,  1.2043e-01,  5.7420e-01, -9.3374e-01,\n",
      "          6.5046e-01,  4.5114e-01, -5.0074e-01,  1.5583e-01, -1.9336e+00,\n",
      "         -5.8301e-01, -1.7728e-01, -1.6426e+00, -5.8958e-01, -2.9550e-01,\n",
      "         -4.3978e-01, -1.2134e-01, -7.8578e-01,  9.9325e-02, -5.4844e-01,\n",
      "         -2.5958e-01,  4.4520e-01,  3.7144e-02, -4.2336e-01,  6.2150e-01,\n",
      "         -9.1455e-01, -5.6493e-01,  7.4641e-01, -3.3112e-01,  2.3593e-01,\n",
      "          1.3328e-01,  1.5587e-01,  1.1190e+00, -1.5495e+00, -1.3596e-01,\n",
      "          1.4079e+00,  2.1847e-01,  1.5264e+00,  1.2588e-01,  2.7382e+00,\n",
      "         -1.6312e-01, -4.4787e-01, -2.8397e-01,  1.1229e+00,  1.2983e+00,\n",
      "         -2.9986e-01,  1.1421e+00,  5.8855e-01, -1.6860e+00, -4.2589e-01,\n",
      "         -1.0989e+00, -1.7140e-01,  8.2011e-01, -1.1842e+00,  1.7617e+00,\n",
      "          6.4392e-01,  1.2209e+00, -9.6025e-01,  4.4884e-01, -7.3592e-01,\n",
      "         -4.6094e-01,  1.2684e+00,  7.4347e-01, -1.1073e+00, -1.6910e-01,\n",
      "          8.9471e-01,  4.6574e-01,  3.2079e-01,  1.1959e-01, -1.0487e+00,\n",
      "          2.2982e-01,  1.5112e+00, -1.3553e-01,  9.5545e-01, -5.7058e-01,\n",
      "          1.6462e+00, -5.2142e-01,  3.5325e-01, -6.4566e-02,  1.9693e+00,\n",
      "          3.3406e-01,  9.0184e-01, -1.4204e+00,  1.4400e+00, -1.1623e+00,\n",
      "         -1.8899e-01, -8.5302e-01, -4.9812e-01, -9.9997e-01, -8.8719e-01,\n",
      "         -1.2752e+00,  7.6607e-01,  3.8080e-01,  1.9152e-02,  1.0127e-01,\n",
      "          8.2566e-01, -1.1457e+00,  6.5159e-01,  6.5239e-01, -1.0488e+00,\n",
      "          5.5658e-01, -1.0826e+00, -1.5867e+00, -1.3716e+00, -4.7003e-01,\n",
      "         -5.1119e-01,  1.6712e+00,  2.9995e-02, -5.8239e-01, -2.5739e-01,\n",
      "          5.2404e-01,  7.3748e-01,  9.4391e-01, -2.2337e+00,  6.1857e-02,\n",
      "          2.7055e-01,  4.5480e-01,  3.3355e+00,  1.6870e+00,  1.3401e+00,\n",
      "          1.6091e-01,  2.0607e+00, -1.9304e+00,  3.0869e-01, -3.0599e-01,\n",
      "         -1.9826e+00,  9.9940e-02,  1.8734e+00, -3.0378e+00, -5.5195e-01,\n",
      "          1.0573e+00, -1.4939e+00, -1.7991e+00, -7.0005e-02, -1.1338e+00,\n",
      "          3.8435e-01, -1.1651e+00, -2.3259e+00,  1.1917e+00, -1.7384e+00,\n",
      "         -1.8033e+00,  1.7230e+00,  1.0858e+00, -1.2535e-01, -1.4840e+00,\n",
      "          1.2223e+00,  1.7225e-02, -1.5353e+00,  8.6296e-01, -5.9559e-01,\n",
      "          8.4074e-01, -1.1337e+00,  1.5730e+00, -1.5920e-02,  1.5390e+00,\n",
      "         -1.0662e+00,  7.3431e-01,  8.4259e-01, -1.7578e+00,  7.0245e-01,\n",
      "         -1.7208e+00,  5.4359e-01,  1.3703e+00, -1.4573e+00,  5.8418e-01,\n",
      "         -1.5548e+00, -5.8997e-01,  7.4221e-01,  3.5578e-01,  7.4525e-01,\n",
      "         -7.8218e-01, -9.9181e-01,  1.7683e+00, -2.8944e-01,  4.8386e-01,\n",
      "         -1.0189e-02,  1.1670e+00, -4.1242e-01,  7.6793e-01, -3.6217e-01,\n",
      "          3.1901e-01, -1.6638e+00, -2.6495e-01,  6.2553e-01,  4.9225e-01,\n",
      "          5.5424e-01, -5.2472e-02,  2.7755e-02,  2.2924e-01,  1.5823e+00,\n",
      "          2.9741e-01,  4.5963e-01, -9.9130e-01,  8.6762e-01, -4.0921e-01,\n",
      "         -1.0233e+00,  1.7452e-01, -3.5376e-01,  2.6286e-01,  5.2128e-01,\n",
      "          1.5680e+00, -1.5945e+00,  6.6118e-01,  7.8269e-01, -5.9981e-01,\n",
      "          9.1714e-01,  1.4729e-01, -4.6446e-03, -7.9230e-01, -1.9008e-01,\n",
      "          7.0652e-01, -6.7144e-02, -1.3937e+00,  6.6343e-01, -5.8929e-01,\n",
      "         -5.6496e-01,  2.0336e+00, -3.1762e-01, -4.6027e-01, -1.2213e+00,\n",
      "          6.4005e-02, -5.3132e-01,  2.1183e-01,  1.3302e-01,  1.1266e+00,\n",
      "         -1.3400e+00, -2.3030e-01,  3.7512e-01,  2.3518e+00, -5.2210e-01,\n",
      "          1.7995e+00,  1.4304e+00,  2.3082e+00, -2.0070e+00, -3.8938e-02,\n",
      "         -1.0753e+00, -1.5751e+00,  4.4150e-01,  7.8469e-01,  7.0369e-01,\n",
      "          1.2326e+00,  5.3970e-01,  9.5406e-01, -1.2451e+00,  1.9964e-01,\n",
      "          8.2688e-01, -2.8333e+00,  1.1685e+00,  1.9500e-01, -6.6297e-01,\n",
      "          1.2431e+00, -2.4096e-01,  7.6506e-01,  6.5795e-02, -6.1074e-01,\n",
      "         -1.7205e+00,  1.3603e+00, -5.2331e-01, -6.6335e-01,  2.1963e-01,\n",
      "          1.5145e-01,  1.0466e+00, -2.9292e-01,  1.2208e+00, -2.1705e-01,\n",
      "         -7.2023e-01,  2.7243e-01, -4.3665e-01, -1.0259e+00,  1.1237e+00,\n",
      "         -1.1093e+00,  7.9087e-01,  8.8822e-01,  1.0275e+00, -7.4205e-01,\n",
      "         -1.1582e+00,  1.1614e+00,  4.8441e-01,  3.6200e-01,  1.2598e+00,\n",
      "          1.4598e-02,  4.8344e-01,  5.5739e-01, -1.5995e+00, -6.2338e-01,\n",
      "         -1.4963e-01,  1.0718e+00,  4.8071e-01, -8.3152e-01,  1.7202e+00,\n",
      "          6.2711e-02, -7.2934e-01,  6.8407e-01,  1.0070e-01, -7.9099e-01,\n",
      "         -2.5580e+00, -1.6174e+00, -1.5141e+00,  2.6437e+00, -1.3821e+00,\n",
      "          8.4439e-01, -1.1604e+00, -9.9775e-02, -6.7781e-01, -6.0564e-01,\n",
      "          1.4827e+00,  1.1379e+00, -3.0742e-01, -1.1034e+00,  9.4756e-01,\n",
      "          4.3548e-01, -1.1530e+00,  2.5682e-01,  3.3504e-01, -3.5841e-01,\n",
      "         -7.3386e-01, -6.2065e-01, -7.8675e-01,  1.3468e+00, -7.1229e-01,\n",
      "          1.1712e+00,  7.9085e-03,  1.4597e+00, -9.3596e-02,  1.5584e-01,\n",
      "         -1.7552e-01,  6.5325e-01, -4.3991e-01,  3.4867e-01,  3.6315e-02,\n",
      "          1.1857e+00,  6.1109e-01, -7.9927e-01,  1.8561e+00, -7.9230e-01,\n",
      "         -8.6668e-02, -3.3904e-01,  1.4995e-01, -4.9163e-01, -1.4793e+00,\n",
      "          9.2364e-01, -1.3012e+00,  1.8050e+00, -7.4183e-01,  6.7401e-01,\n",
      "         -1.4959e+00, -2.9218e-03, -1.0244e+00, -3.0040e-01, -3.1280e-01,\n",
      "         -6.1887e-01,  4.1290e-01, -1.8405e+00,  6.3166e-01, -2.5522e-01,\n",
      "          6.6262e-01, -1.3517e+00,  1.0665e+00, -1.0145e-01,  2.8742e-01,\n",
      "         -2.7559e-01, -1.5286e+00,  2.9668e-02, -3.5212e-01,  1.5411e+00,\n",
      "         -2.6600e-01,  7.6615e-01,  5.2760e-01, -1.0923e+00, -1.3105e+00,\n",
      "          1.4914e+00, -8.9256e-01,  2.8636e-01, -1.0110e+00, -3.9418e-01,\n",
      "          3.9692e-01,  6.3545e-01, -2.8541e-01,  4.6138e-01,  2.4605e-01,\n",
      "         -1.0360e+00,  1.5436e-02,  8.6420e-01, -1.5501e+00,  3.9368e-01,\n",
      "          1.8062e+00,  3.6915e-02, -2.5089e+00,  2.5543e-02,  3.7463e-01,\n",
      "          2.0053e-01,  5.1718e-01,  2.5755e-01, -1.6705e-01,  1.0656e+00,\n",
      "         -7.0481e-01, -9.3606e-02,  1.6028e-02, -1.2788e+00,  2.1535e-01,\n",
      "          7.1754e-01,  2.8043e-01, -8.1697e-01, -1.3175e-01, -5.5516e-01,\n",
      "         -2.3300e-01, -8.5055e-01,  4.9442e-01,  7.7835e-01, -1.0877e+00,\n",
      "          2.1265e+00,  4.1477e-02,  9.6980e-01,  7.0334e-02, -1.1364e+00,\n",
      "         -4.0342e-01, -5.8432e-02, -1.0592e-01,  1.9029e+00, -4.3068e-03,\n",
      "         -7.5410e-01,  1.7472e+00,  1.3179e+00, -1.6127e+00, -4.3806e-01,\n",
      "          9.8071e-01, -1.8565e+00,  3.7178e-01, -8.3402e-02, -1.9087e+00,\n",
      "         -5.6165e-01, -7.7318e-01,  1.7396e+00,  9.0652e-01,  9.4068e-01,\n",
      "         -4.2775e-01,  1.2019e+00,  2.4697e+00,  1.3242e+00, -5.5238e-01,\n",
      "          1.4441e+00,  1.4610e+00,  2.4963e+00, -1.9626e+00,  2.8342e-01,\n",
      "          7.9591e-01, -3.8065e-01,  2.1753e+00,  1.0783e+00,  5.0872e-01,\n",
      "          2.5079e+00,  1.3680e+00, -1.6238e+00,  1.5635e-01, -3.4258e-02,\n",
      "          7.0365e-01,  3.7513e-01,  5.3884e-01, -4.5646e-01, -6.4532e-02,\n",
      "         -3.7341e-01, -1.7709e-02, -4.7744e-01, -5.2998e-02, -1.2120e-01,\n",
      "         -4.2589e-03,  2.7407e-01,  3.5743e-01,  1.2335e+00, -1.0625e-01,\n",
      "          2.3510e-01,  8.7616e-01, -1.1191e+00,  8.5975e-01,  1.8609e+00,\n",
      "         -1.3809e+00,  9.9740e-01, -2.9971e-01,  1.6536e+00, -1.1117e+00,\n",
      "         -9.1350e-01, -2.2185e-01, -7.7252e-01,  9.6990e-01, -1.7544e-01,\n",
      "          3.2135e-01,  8.5300e-01, -1.0169e+00,  2.3924e-01,  5.8048e-01,\n",
      "         -2.2043e-01, -8.4466e-01, -3.8940e-01,  2.2475e-01,  2.2955e-01,\n",
      "          5.6493e-01,  1.0333e+00,  1.4753e+00, -5.6152e-01,  2.9582e-01,\n",
      "         -1.7137e+00, -3.7163e-01, -4.3266e-01, -1.3840e-01, -1.2555e+00,\n",
      "         -2.2829e+00,  1.1740e+00,  2.5506e+00, -1.6304e-01,  1.8840e-01,\n",
      "          3.1650e-01,  9.4738e-01, -3.5767e-01, -8.6917e-01, -1.8876e+00,\n",
      "          6.4954e-01,  1.5174e+00,  6.9973e-01,  1.0140e+00,  4.8604e-01,\n",
      "          7.3292e-01, -1.9017e+00, -9.9853e-01, -1.1049e+00,  8.1710e-01,\n",
      "          2.1680e+00,  2.0904e+00,  7.2788e-01, -1.7386e-01, -7.3576e-01,\n",
      "         -5.5553e-01,  3.7980e-01,  8.0350e-01,  6.4870e-01, -8.4193e-01,\n",
      "         -2.7868e-01,  1.1122e+00, -1.1291e+00,  1.0973e+00, -1.8271e-01,\n",
      "         -1.3269e+00, -7.4517e-01,  2.6651e-01,  8.5665e-01, -1.9379e+00,\n",
      "         -8.0929e-01, -9.4848e-01,  9.0699e-01, -7.5491e-02,  7.3453e-01,\n",
      "         -2.7346e-01, -5.6971e-01,  9.9929e-01, -9.7497e-01,  8.3498e-01,\n",
      "         -1.3429e-02,  1.5926e+00,  2.0049e+00,  5.1643e-01,  6.6867e-01,\n",
      "         -1.2399e-01,  3.8761e-01, -3.6236e-01, -1.6242e+00,  3.8185e-01,\n",
      "          1.1120e+00,  3.0826e-01, -5.5197e-01,  4.0868e-01, -1.3057e+00,\n",
      "          1.3587e+00,  7.2369e-01, -1.2949e+00,  7.9567e-01,  1.6046e+00,\n",
      "         -6.2145e-01, -1.2458e+00, -5.9183e-01, -1.6256e+00, -8.9752e-01,\n",
      "         -5.5043e-01, -1.3930e+00, -5.9607e-02, -2.8749e-01, -1.8525e-01,\n",
      "          1.4968e+00, -2.2887e+00,  1.6092e+00, -1.7545e-02, -9.3851e-01,\n",
      "         -1.3052e+00,  8.8713e-01,  3.1053e-01,  1.2612e-02,  1.5890e+00,\n",
      "          1.6754e+00,  2.8363e-01,  1.4782e+00, -1.6896e+00, -7.4086e-01,\n",
      "         -6.3770e-01, -1.1209e+00, -1.3906e+00, -6.0696e-01,  6.2405e-01,\n",
      "         -1.2842e+00,  9.0536e-01,  1.1771e-01,  6.5148e-01,  3.1728e-02,\n",
      "         -3.6343e-01,  1.9311e+00,  7.0904e-01, -1.0150e+00, -3.6893e-01,\n",
      "          1.1026e+00, -1.5171e-01,  2.4521e+00, -7.2093e-02, -1.9779e-01,\n",
      "          4.4299e-01,  4.3638e-01, -1.9969e+00,  1.3472e+00, -1.7697e-01,\n",
      "          7.7787e-01,  4.2401e-01, -4.5639e-01, -8.0101e-02,  2.2453e-01,\n",
      "         -1.1065e+00,  2.1690e+00, -1.2753e+00,  7.7506e-01,  1.1603e+00,\n",
      "         -7.5356e-01, -1.1102e-01, -1.0583e+00,  3.2914e-01, -8.8374e-01,\n",
      "         -1.1353e+00, -1.0524e+00, -9.7974e-02,  2.6116e-01,  9.6784e-01,\n",
      "          1.1693e+00, -1.6062e+00,  1.4960e+00, -4.5533e-01,  6.5606e-01,\n",
      "         -3.3964e-01, -1.4881e-01, -8.9069e-01,  6.4462e-01,  2.4084e-01,\n",
      "         -1.3543e-02,  1.6910e+00,  3.6845e-01, -2.7313e-01,  8.9757e-01,\n",
      "          7.2507e-01, -6.5294e-01, -7.4549e-01,  1.0982e+00,  6.3860e-01,\n",
      "          5.0914e-02, -1.6309e+00, -1.9122e-01, -1.3078e-01, -6.6195e-02,\n",
      "         -1.0688e+00,  1.7027e+00,  8.8002e-01,  1.2057e+00, -1.2960e-01,\n",
      "          2.4411e-01, -1.0211e+00, -9.6593e-01,  1.3918e+00,  2.3454e+00,\n",
      "          2.8152e-02, -9.9804e-02,  5.1401e-01, -1.0012e+00,  1.2097e+00,\n",
      "          5.0147e-02, -1.1100e+00,  3.3992e-01, -2.6027e-01, -1.4928e-02]],\n",
      "       device='cuda:0'), 'linear2.parametrizations.weight.0.lora_B': tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0'), 'linear2.parametrizations.weight.1.lora_A': tensor([[-5.3778e-01,  5.0062e-01, -2.5248e-01, -4.6073e-01, -1.6402e+00,\n",
      "         -1.2988e+00,  3.3184e-01, -1.5888e+00,  1.0011e+00,  1.4593e+00,\n",
      "          7.0265e-01,  1.4784e-01,  1.1435e+00,  5.3389e-01, -7.2500e-02,\n",
      "          5.1107e-01,  6.1614e-02,  4.6636e-01,  4.5238e-01, -1.0453e+00,\n",
      "          6.6374e-01,  6.2754e-01,  1.9711e+00,  1.9313e+00,  7.3585e-02,\n",
      "         -4.0139e-01,  9.8828e-01, -1.5375e+00,  5.7818e-01,  1.0009e+00,\n",
      "          1.3586e+00,  4.2985e-01, -5.5745e-01,  1.3096e+00, -1.3956e+00,\n",
      "          1.0057e+00,  1.6482e-01, -2.4725e-01, -2.8635e-01,  1.1354e+00,\n",
      "          1.7797e-01, -1.1007e+00, -2.0159e+00,  1.0329e-01, -1.0445e+00,\n",
      "          1.3124e+00,  1.0662e+00, -4.5046e-01,  1.6864e+00,  7.4168e-01,\n",
      "          8.7000e-01, -1.0857e+00,  1.4777e+00, -1.1858e-01, -6.9729e-01,\n",
      "          9.7455e-01,  2.8264e-01, -3.0975e-01, -1.9914e+00, -1.1169e+00,\n",
      "         -4.1912e-01,  1.3623e+00,  6.0889e-01, -1.1305e+00,  6.4235e-01,\n",
      "         -3.0998e-01,  7.5232e-01,  3.8002e-01,  6.8726e-01,  3.1985e-01,\n",
      "          9.5248e-01,  6.8698e-01, -3.8010e-01, -1.4771e+00,  3.4988e-01,\n",
      "          4.9042e-01, -7.4524e-01,  1.2089e-01, -1.1584e+00, -1.0927e+00,\n",
      "          8.3964e-01,  3.4229e-01,  7.0994e-01, -1.9606e+00, -1.7912e-01,\n",
      "         -6.9223e-03, -1.6114e-01,  1.3183e+00, -4.4362e-01,  3.5650e-02,\n",
      "         -9.0562e-01,  1.3941e+00,  6.5865e-01, -5.8277e-01,  1.2450e+00,\n",
      "         -1.5906e-01,  1.4535e-01,  5.0591e-01, -6.6845e-01, -2.4893e+00,\n",
      "          1.9810e+00,  1.1133e+00,  8.6645e-01, -8.9282e-01, -2.8525e-01,\n",
      "          1.6049e-01, -1.3958e+00,  1.1953e+00, -1.2016e+00,  3.7219e-01,\n",
      "         -6.3853e-01, -7.9155e-01,  6.1446e-01, -1.8673e+00, -3.2631e-01,\n",
      "          1.0647e+00,  6.6672e-01,  4.4673e-01,  1.1356e+00, -3.5812e-01,\n",
      "         -2.3384e-01, -1.6808e-01,  3.9425e-01, -8.2369e-02, -9.3824e-01,\n",
      "          6.1238e-01,  6.5679e-02,  3.7962e-01,  7.3093e-01,  1.1096e+00,\n",
      "         -7.3398e-01, -4.8620e-01, -1.2697e+00,  5.5875e-01, -5.4644e-01,\n",
      "         -5.6945e-01, -2.6561e-01,  5.8647e-01,  8.9823e-01,  1.6915e+00,\n",
      "          1.0882e+00, -2.5985e-01, -6.7265e-01,  1.2047e+00,  1.7878e-01,\n",
      "          3.0623e-01,  6.0046e-01,  9.7645e-01,  1.0214e+00, -2.2109e+00,\n",
      "         -5.6957e-02, -1.0378e+00, -4.7614e-01,  3.7565e-01, -6.6929e-01,\n",
      "         -4.1266e-01,  7.1187e-01,  4.7895e-01,  5.2897e-01, -3.6535e-01,\n",
      "          1.2387e+00, -1.5787e+00, -1.3908e-01, -8.3135e-01, -1.1773e+00,\n",
      "          6.2325e-02,  2.2802e-01,  4.7913e-01, -1.8465e-01, -7.1249e-02,\n",
      "         -5.3484e-01, -3.7343e-02, -2.9141e-01, -1.6211e+00, -5.5914e-01,\n",
      "         -4.7897e-01,  6.3386e-01, -9.2844e-02, -1.9751e+00, -2.9447e-01,\n",
      "          4.9580e-01,  2.4351e+00, -1.5059e+00, -6.4860e-01,  4.4293e-02,\n",
      "          7.2968e-01, -6.8768e-01, -2.8924e-01,  2.9804e+00, -8.6584e-01,\n",
      "         -1.0632e+00, -6.6198e-01, -3.5378e-01, -6.5428e-01,  1.0256e-02,\n",
      "         -6.3702e-01,  7.0615e-01,  8.4091e-01,  7.0268e-01, -4.4257e-01,\n",
      "          1.1251e+00,  9.8794e-01,  6.0738e-01,  8.5263e-01, -9.9797e-01,\n",
      "          1.0642e-01,  7.3407e-01, -1.0515e+00, -3.1155e-01,  5.9012e-01,\n",
      "          2.3275e+00,  2.9579e-01, -3.9662e-01, -6.8478e-01,  8.8610e-02,\n",
      "          1.4397e+00,  8.9137e-01,  1.4765e+00,  6.2753e-01, -8.0036e-01,\n",
      "          9.2439e-01, -1.5460e+00, -2.6397e+00,  2.0442e+00, -1.6868e+00,\n",
      "          2.8924e-01,  9.6167e-01, -2.7430e-03,  3.5508e-01,  1.6911e+00,\n",
      "         -1.4485e+00,  1.3266e-01,  1.4263e+00,  2.6935e-01,  1.9679e+00,\n",
      "          9.0906e-02, -6.2201e-01,  1.1388e+00, -1.5441e-01,  1.5831e+00,\n",
      "          6.5424e-01,  1.9682e-01,  4.0479e-01, -9.8963e-01,  9.4911e-02,\n",
      "         -5.5147e-02, -2.3000e-01, -4.3465e-01, -5.5221e-01,  4.7386e-01,\n",
      "          1.2099e+00, -8.2468e-02, -1.5419e+00, -8.6615e-02, -5.3907e-01,\n",
      "          5.6309e-02, -5.0502e-01,  1.2513e+00,  2.3962e-01,  8.6335e-01,\n",
      "          1.2644e-01,  1.2020e+00, -1.9214e+00, -4.1811e-02,  3.8765e-01,\n",
      "          1.2866e+00, -1.3505e+00,  1.4790e+00,  5.5135e-02, -1.6250e+00,\n",
      "          1.9851e-01, -5.6036e-01,  1.1743e+00,  1.4200e+00,  5.7968e-01,\n",
      "          6.3955e-01, -7.2707e-01, -1.6015e+00,  9.0285e-01, -1.0105e+00,\n",
      "         -6.8237e-01, -5.9624e-01,  8.9809e-02,  7.1191e-01,  5.9989e-01,\n",
      "          1.5976e+00, -1.0585e+00,  1.8741e-01, -2.3304e-01, -6.5449e-01,\n",
      "         -3.4611e-01,  2.8855e-01, -6.9327e-01, -2.4405e-02,  4.2118e-01,\n",
      "          8.8052e-01,  6.9900e-01, -8.9736e-01, -4.6194e-01, -2.3685e+00,\n",
      "          9.0665e-01,  1.4788e-02, -1.3978e+00,  1.1298e-01,  1.0343e+00,\n",
      "         -1.1467e+00, -7.2315e-01, -4.6647e-01,  4.2388e-01, -3.5911e-01,\n",
      "          4.6823e-01,  4.1812e-01,  1.1716e+00,  1.5724e+00, -1.7535e+00,\n",
      "          8.8127e-01, -5.5646e-01, -1.5462e+00, -5.9610e-03, -7.9178e-01,\n",
      "          2.6904e-01,  9.7480e-01,  6.9601e-01,  1.5301e+00, -2.8609e-01,\n",
      "          8.7733e-01,  1.9847e+00,  7.2564e-02,  1.9762e-01,  5.6343e-01,\n",
      "         -6.7038e-01,  1.3757e+00,  4.8853e-01,  6.2112e-01,  1.2332e-02,\n",
      "         -1.0337e+00, -1.4536e+00, -8.7258e-01,  9.7678e-01,  5.1517e-01,\n",
      "         -1.2836e-01, -7.2414e-01, -1.5340e+00, -1.1906e+00, -7.1561e-01,\n",
      "          6.1620e-02,  4.7682e-01, -3.4793e-01,  3.9947e-01,  1.0243e+00,\n",
      "          2.2221e+00,  4.0900e-01,  1.0616e+00, -5.2447e-01, -1.5464e-02,\n",
      "         -1.0108e+00, -1.4981e+00, -7.7034e-01, -1.4001e+00, -1.0505e+00,\n",
      "         -8.1586e-01, -6.6731e-01,  2.7175e-01,  1.4630e+00, -4.3978e-02,\n",
      "         -8.3942e-01,  8.7744e-02, -1.0348e+00,  8.1322e-01, -2.1203e-01,\n",
      "          8.9416e-02,  1.1058e+00, -1.7656e-01,  8.9135e-01,  9.2527e-02,\n",
      "         -9.5703e-02,  1.4807e+00,  3.7497e-01,  6.1688e-01,  2.0405e+00,\n",
      "         -1.3228e+00,  1.1492e-01,  4.1343e-02,  5.9607e-01,  5.6352e-02,\n",
      "          1.2886e+00,  7.2711e-01, -8.3209e-01,  7.7852e-01, -8.8301e-01,\n",
      "         -1.1110e-02,  1.0420e-01,  1.8986e-01, -5.4325e-02,  5.9998e-01,\n",
      "         -5.1507e-01,  1.1942e+00,  6.4759e-01,  1.1035e+00, -1.3896e+00,\n",
      "          2.2299e-01,  4.3832e-01, -4.3442e-01, -7.0159e-01, -1.5203e-01,\n",
      "         -1.0689e+00,  1.9790e-01,  1.6309e+00,  9.0630e-01, -2.3515e-01,\n",
      "         -3.1339e-01, -5.3596e-01,  9.2257e-01, -9.1641e-01, -9.7513e-02,\n",
      "          4.7767e-01,  4.7560e-01,  4.1090e-01,  9.6750e-01,  2.2614e+00,\n",
      "         -6.3239e-01, -2.3385e-01, -5.4517e-01,  4.2653e-01,  6.4872e-01,\n",
      "         -1.1861e+00, -1.0666e+00, -9.4154e-01, -6.4502e-01,  4.3041e-02,\n",
      "         -8.3422e-01,  1.0601e-01,  1.6626e+00,  1.8214e+00, -1.2967e+00,\n",
      "          1.9058e+00, -7.4369e-02, -1.4682e+00, -1.9847e+00, -9.6314e-01,\n",
      "         -7.0851e-01,  2.9572e+00,  5.4656e-01,  6.4765e-01,  1.0512e+00,\n",
      "         -5.0199e-01,  1.3666e+00,  1.3567e+00,  3.1865e-01, -1.5884e+00,\n",
      "          1.3070e+00, -1.3004e+00, -1.2574e+00,  1.0998e-01, -7.0100e-01,\n",
      "         -2.2554e+00, -1.3226e+00,  6.1742e-01,  7.2662e-02,  8.7798e-01,\n",
      "          3.9551e-01, -5.5556e-01,  1.3802e+00, -3.7840e-01,  1.2715e+00,\n",
      "          1.5293e-01,  3.3730e-01,  1.3252e+00, -3.0941e-02, -4.3958e-01,\n",
      "          3.1633e-01,  1.8323e+00, -2.1608e+00,  1.2766e+00, -1.9685e+00,\n",
      "          8.8180e-01, -1.3533e-01,  7.8503e-01,  7.6454e-01,  3.5082e-01,\n",
      "          2.5944e-01, -2.7476e-01, -8.1680e-01,  6.5851e-01, -1.1064e-01,\n",
      "         -3.0470e-01, -9.4929e-01,  2.0571e+00, -3.4579e-03,  3.2891e-01,\n",
      "          3.7356e-01, -2.9107e-02,  4.2598e-01, -5.8684e-01,  1.0023e+00,\n",
      "          6.8796e-01, -8.5898e-01, -5.2759e-01,  1.0208e+00, -4.0551e-01,\n",
      "          1.5986e-01, -8.6342e-01,  4.8315e-01,  9.4724e-01, -2.3937e-01,\n",
      "          9.8492e-01, -1.2190e+00,  3.1031e-01, -1.0853e+00, -1.9087e-01,\n",
      "          3.6596e-01,  5.8814e-01, -1.3911e+00,  1.2334e+00,  9.2944e-01,\n",
      "         -1.1043e+00,  1.7679e+00, -9.3963e-01,  3.0339e+00, -1.1074e+00,\n",
      "         -5.6888e-01,  1.3115e+00, -2.0277e+00, -3.8062e-01, -6.0343e-01,\n",
      "          9.7891e-01, -7.9466e-01,  1.5427e+00,  1.6916e+00, -2.9359e-01,\n",
      "         -9.5279e-01,  9.3344e-01,  7.5676e-02,  1.3537e-01,  1.2023e+00,\n",
      "         -8.5832e-02, -4.5829e-01,  3.6731e-02, -1.7174e-01,  2.0160e-01,\n",
      "          9.0425e-01,  3.1949e-01, -1.2663e+00, -1.6377e+00,  2.2632e+00,\n",
      "         -3.0603e-01,  4.2586e-01, -2.1044e-01, -1.3177e-01, -4.5362e-01,\n",
      "         -9.3586e-01, -8.1803e-01,  3.2496e-01,  1.0687e+00, -3.0248e-01,\n",
      "          1.1690e+00,  2.7800e-01, -4.2404e-01, -1.6740e+00, -6.6589e-01,\n",
      "         -6.4550e-02,  6.9095e-01,  7.2700e-01, -1.4834e+00, -1.1887e+00,\n",
      "          1.2637e+00,  7.9706e-01, -9.3190e-01, -2.9665e-01,  1.1387e+00,\n",
      "         -8.5271e-02, -1.9874e-01, -1.2179e+00, -6.4699e-01, -1.5318e+00,\n",
      "         -1.9546e+00, -6.6284e-01, -3.9955e-01, -1.0575e+00, -7.5432e-01,\n",
      "          1.0765e+00,  1.0911e+00, -1.2498e+00, -9.8145e-01,  1.0646e+00,\n",
      "          7.4172e-01,  3.7034e-01, -2.6712e-01,  1.2664e-01,  2.1573e-01,\n",
      "          1.1480e-01, -1.4008e-02, -1.3542e+00,  4.7126e-01,  2.7181e-01,\n",
      "         -1.5395e-02, -7.7334e-01, -6.2269e-01, -7.1572e-01,  1.6405e-01,\n",
      "         -1.7611e+00, -4.5138e-01,  1.2649e-01, -9.3920e-01,  1.3229e+00,\n",
      "         -1.4775e-02,  1.5050e+00, -2.1804e-01, -3.9862e-02, -1.9017e-01,\n",
      "          1.4785e+00, -4.2695e-01, -5.0929e-01,  4.7094e-01,  2.4593e+00,\n",
      "          6.1117e-01, -1.0835e+00, -6.8726e-01, -6.5065e-01,  3.2376e-01,\n",
      "          1.2060e+00, -2.0928e-01, -3.8546e-01, -9.5960e-01, -1.3330e+00,\n",
      "         -4.5470e-01, -7.3051e-01,  1.0448e+00,  6.3383e-01, -2.2542e+00,\n",
      "         -7.2649e-01, -1.7028e-02, -5.0208e-01,  1.7604e-01, -1.2267e+00,\n",
      "         -6.5230e-01, -1.3518e+00,  1.0733e+00, -7.2634e-01,  1.4990e+00,\n",
      "         -3.0805e-01, -6.7817e-01,  6.1753e-01,  1.3513e+00,  7.3618e-01,\n",
      "         -7.2117e-01, -2.1885e+00, -1.5641e+00, -3.7717e-01, -1.0351e+00,\n",
      "         -5.9891e-01,  9.7182e-01, -1.5187e+00, -1.2555e+00,  7.7871e-01,\n",
      "          1.4136e+00, -3.4037e-01, -1.8027e+00, -1.6025e-01,  1.5480e+00,\n",
      "         -1.9912e+00,  1.2724e-01, -1.3661e+00, -3.3977e-01,  3.5617e-02,\n",
      "         -4.3862e-01,  3.0221e-02,  1.4411e-01, -2.4252e-01, -1.7509e-01,\n",
      "          2.1297e-01, -1.0849e+00, -7.9118e-01,  1.3877e+00, -7.5618e-01,\n",
      "          5.6427e-01,  2.9463e-01,  4.1491e-01, -1.1419e+00, -1.5411e+00,\n",
      "          3.9600e-01,  2.4005e-01, -1.0359e+00,  5.7936e-01,  4.3669e-01,\n",
      "         -4.2907e-01,  2.0502e-01,  8.6126e-01,  6.7991e-01, -1.0043e+00,\n",
      "          8.4199e-01, -4.3049e-01,  1.6415e+00, -1.3322e+00,  1.8686e+00,\n",
      "         -8.9310e-01,  1.0785e-01,  7.3082e-03,  1.6945e+00, -1.7267e-01,\n",
      "         -1.0647e+00,  4.9015e-01, -2.0943e-01, -1.3841e-01, -6.5667e-01,\n",
      "          6.5551e-01, -6.4612e-01, -7.7717e-02, -6.9471e-01, -4.9447e-01,\n",
      "         -7.4178e-01, -2.3803e-02, -7.3864e-01,  7.6088e-01, -2.1781e-01,\n",
      "          7.1106e-01,  1.3698e+00,  1.6950e+00,  1.2691e+00, -1.6551e-01,\n",
      "         -1.2075e+00,  2.6484e-01,  8.0415e-01,  1.7090e-01, -3.4507e-01,\n",
      "          1.2481e+00, -1.1044e+00,  8.8908e-01,  1.3877e-01, -1.9557e-01,\n",
      "          5.9116e-01, -2.3643e-01,  3.5643e-01,  9.9498e-01,  1.2589e+00,\n",
      "         -1.2974e+00, -1.0144e+00,  1.1427e-01,  8.3994e-01, -2.5169e+00,\n",
      "         -1.4555e+00, -1.6264e+00, -7.6633e-01, -1.6691e+00,  1.7348e+00,\n",
      "          1.2997e+00, -6.9728e-01, -9.2607e-02, -1.4507e+00, -7.9898e-01,\n",
      "         -6.3049e-01, -1.6218e+00,  4.4022e-01, -2.1857e+00,  1.2742e+00,\n",
      "          1.1566e+00, -1.5157e+00,  5.0857e-01, -6.3633e-01, -4.7579e-03,\n",
      "         -1.6653e+00, -1.5308e+00, -1.3894e+00, -3.1755e-01, -1.1561e+00,\n",
      "         -4.9814e-01, -3.0469e-01,  1.3438e+00,  1.5559e+00, -1.3661e-02,\n",
      "          2.7310e+00, -2.8260e+00,  8.1015e-01,  7.7073e-01,  8.3337e-01,\n",
      "          3.1673e-02,  4.2774e-01, -1.1690e+00, -1.3617e-01, -8.7281e-01,\n",
      "         -5.4226e-01, -1.5558e+00,  2.9024e-01, -1.2978e+00,  3.6397e-01,\n",
      "         -2.9855e+00, -1.3884e+00,  1.2387e+00,  8.0705e-01,  1.9849e+00,\n",
      "          1.0508e+00, -8.0657e-01,  1.4287e-01, -1.4384e+00,  4.3642e-02,\n",
      "          4.5943e-01,  1.0698e+00,  2.0999e-01,  1.6363e+00, -1.1071e-01,\n",
      "          5.3315e-01, -1.6014e+00,  1.5167e-01, -9.0445e-01, -5.8487e-01,\n",
      "          1.2325e+00,  7.7122e-01,  9.2839e-01, -4.8078e-01,  9.8383e-01,\n",
      "         -9.8345e-02, -5.3525e-01, -1.9397e+00, -9.5719e-01,  6.1110e-01,\n",
      "          1.1751e+00, -7.9875e-01, -6.2302e-01, -9.4380e-01,  3.3364e-01,\n",
      "         -5.6995e-01, -1.8357e+00,  8.9232e-02, -1.1509e+00,  1.5331e+00,\n",
      "         -1.4341e-01, -1.4170e+00,  1.5939e+00, -8.3390e-01,  8.3684e-02,\n",
      "         -1.5952e-01,  8.0358e-01,  1.9663e-01,  1.4934e-01,  1.5884e+00,\n",
      "         -9.5795e-01,  5.2548e-01,  4.2353e-01, -2.4482e-01,  8.0049e-01,\n",
      "          8.8338e-01, -5.0344e-02,  1.4234e+00, -7.0305e-01,  1.6758e+00,\n",
      "          7.6229e-01,  6.9322e-01, -6.6144e-02, -1.5320e+00, -1.5542e+00,\n",
      "         -3.1211e-01,  1.3013e+00, -4.4724e-02, -5.5008e-01, -1.3935e+00,\n",
      "         -1.0106e+00,  1.0891e+00, -1.4459e+00,  7.1094e-01, -2.3074e-01,\n",
      "          1.4164e-01, -3.4978e-02,  2.8180e-01, -6.4215e-01, -3.8189e-01,\n",
      "         -8.3138e-01,  1.9786e+00,  1.6286e+00, -1.9286e-01,  5.8190e-01,\n",
      "         -1.2230e+00, -8.5819e-01, -1.5681e+00,  1.1752e+00, -1.1751e+00,\n",
      "         -1.4612e+00, -4.4186e-01, -6.9349e-01, -6.0069e-01,  6.3881e-01,\n",
      "         -2.8157e-01,  2.1631e+00, -6.2762e-01,  1.1583e+00, -2.5098e-01,\n",
      "         -2.4578e-01,  1.2244e-01, -4.2268e-01, -1.2509e-01,  5.5669e-01,\n",
      "         -1.2806e+00,  4.9846e-01,  2.3806e-01,  6.8873e-01,  1.1276e+00,\n",
      "         -1.0639e+00, -9.0684e-01,  1.0001e+00, -1.4958e-01, -7.6581e-01,\n",
      "         -1.8987e+00, -4.4927e-01, -1.3598e-01, -5.2350e-01,  4.1639e-01,\n",
      "         -1.2096e+00, -7.2565e-02,  9.2014e-01,  1.0451e+00, -1.2786e+00,\n",
      "          4.1124e-01, -4.5565e-01,  1.6041e+00, -1.3782e+00, -1.0604e+00,\n",
      "          9.5142e-01, -8.0264e-01,  4.9259e-01,  9.7785e-01, -2.1489e-01,\n",
      "         -7.3380e-02,  4.7762e-01,  1.0401e+00, -3.9330e-01,  2.3120e+00,\n",
      "         -1.1336e-01, -9.7859e-02,  9.6752e-01,  1.5829e+00,  5.4239e-01,\n",
      "          2.2163e-01, -1.2979e+00,  3.6690e-01, -3.6309e-01,  1.4173e-01,\n",
      "          1.2557e+00, -3.0005e-01, -8.2970e-01, -9.0368e-01, -2.5526e-01,\n",
      "         -1.9876e+00,  1.1303e+00, -3.4795e-01,  1.5564e-01,  5.5985e-01,\n",
      "          1.3933e-01, -8.8665e-01, -8.0183e-01,  3.7564e-01,  1.0475e-01,\n",
      "          2.6808e-01,  4.7170e-02,  8.5288e-01, -1.4023e+00,  5.7721e-01,\n",
      "          2.8124e-01, -4.8543e-01, -3.6033e-01,  7.2159e-01,  1.7574e-01,\n",
      "          1.4751e-01,  8.2196e-01, -1.0010e+00, -3.9320e-01,  2.1563e+00,\n",
      "          4.4837e-01, -4.9456e-01, -9.1665e-01,  7.9998e-01, -2.7311e-01,\n",
      "         -1.5931e+00,  2.4600e-01, -3.2590e-01, -3.3455e-01, -3.0818e-01,\n",
      "         -8.9066e-01, -9.3536e-01,  2.5709e+00, -7.6473e-01,  1.1543e+00,\n",
      "         -9.7293e-01, -7.2033e-01,  6.0942e-01,  5.4267e-01,  1.2278e-01,\n",
      "          2.7542e-01,  1.6179e+00, -5.6778e-03,  7.7443e-01, -4.9752e-01,\n",
      "          1.2757e+00,  1.9824e+00, -4.8389e-02, -2.1898e+00,  1.7971e-01,\n",
      "          1.3173e+00, -2.5388e-01, -1.3782e+00, -1.4508e-01, -5.5827e-01]],\n",
      "       device='cuda:0'), 'linear2.parametrizations.weight.1.lora_B': tensor([[5.1691e-24],\n",
      "        [6.9704e-24],\n",
      "        [7.0480e-24],\n",
      "        ...,\n",
      "        [0.0000e+00],\n",
      "        [0.0000e+00],\n",
      "        [5.4178e-24]], device='cuda:0'), 'linear3.bias': tensor([-0.0103, -0.0212, -0.0082, -0.0111,  0.0080,  0.0037, -0.0220,  0.0151,\n",
      "        -0.0142,  0.0156], device='cuda:0'), 'linear3.parametrizations.weight.original': tensor([[-0.0093, -0.0065, -0.0171,  ..., -0.0003, -0.0218, -0.0019],\n",
      "        [-0.0040,  0.0043, -0.0117,  ..., -0.0091,  0.0090, -0.0100],\n",
      "        [ 0.0135, -0.0127, -0.0042,  ..., -0.0279,  0.0145, -0.0176],\n",
      "        ...,\n",
      "        [-0.0055,  0.0122, -0.0261,  ..., -0.0229, -0.0052, -0.0048],\n",
      "        [-0.0077, -0.0225, -0.0045,  ...,  0.0035, -0.0017,  0.0107],\n",
      "        [ 0.0243,  0.0129,  0.0088,  ..., -0.0081, -0.0131,  0.0158]],\n",
      "       device='cuda:0'), 'linear3.parametrizations.weight.0.lora_A': tensor([[-1.1948,  0.2091, -0.6772,  ...,  0.5745,  1.0240,  0.5501]],\n",
      "       device='cuda:0'), 'linear3.parametrizations.weight.0.lora_B': tensor([[4.4642e-38],\n",
      "        [4.2414e-38],\n",
      "        [4.4642e-38],\n",
      "        [4.2414e-38],\n",
      "        [4.2414e-38],\n",
      "        [4.7734e-38],\n",
      "        [4.2414e-38],\n",
      "        [4.3902e-38],\n",
      "        [4.2414e-38],\n",
      "        [0.0000e+00]], device='cuda:0'), 'linear3.parametrizations.weight.1.lora_A': tensor([[ 0.2270, -1.8392,  0.8899,  ...,  1.6216, -0.0716,  0.4275]],\n",
      "       device='cuda:0'), 'linear3.parametrizations.weight.1.lora_B': tensor([[1.6782e-22],\n",
      "        [9.1051e-23],\n",
      "        [1.2500e-22],\n",
      "        [9.4563e-23],\n",
      "        [1.1323e-22],\n",
      "        [1.5730e-22],\n",
      "        [8.5730e-23],\n",
      "        [8.6588e-23],\n",
      "        [7.7057e-23],\n",
      "        [0.0000e+00]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "original_weights = {}\n",
    "\n",
    "for name , param in net.named_parameters():\n",
    "\n",
    "    original_weights[name] = param.clone().detach()\n",
    "\n",
    "print(original_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 564.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.101\n",
      "wrong counts for the digit 0: 980\n",
      "wrong counts for the digit 1: 1135\n",
      "wrong counts for the digit 2: 1032\n",
      "wrong counts for the digit 3: 1010\n",
      "wrong counts for the digit 4: 982\n",
      "wrong counts for the digit 5: 892\n",
      "wrong counts for the digit 6: 958\n",
      "wrong counts for the digit 7: 1028\n",
      "wrong counts for the digit 8: 974\n",
      "wrong counts for the digit 9: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in tqdm(test_loader , desc='Testing'):\n",
    "\n",
    "                x,y = data\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                output = net(x.view(-1,784))\n",
    "\n",
    "                for idx , i in enumerate(output):\n",
    "\n",
    "                    if torch.argmax(i) == y[idx]:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        wrong_counts[y[idx]] +=1\n",
    "                    total+=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets calculate the total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
      "Total number of parameters: 2,807,010\n"
     ]
    }
   ],
   "source": [
    "total_parameters_original = 0\n",
    "\n",
    "for index , layer in enumerate([net.linear1 , net.linear2 , net.linear3]):\n",
    "\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "\n",
    "print(f'Total number of parameters: {total_parameters_original:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets define LoRA parameterization : https://pytorch.org/tutorials/intermediate/parametrizations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "        super().__init__()\n",
    "        # Section 4.1 of the paper: \n",
    "        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "        \n",
    "        # Section 4.1 of the paper: \n",
    "        #   We then scale ∆Wx by α/r , where α is a constant in r. \n",
    "        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n",
    "        #   As a result, we simply set α to the first r we try and do not tune it. \n",
    "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # Return W + (B*A)*scale\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
    "\n",
    "    # From section 4.2 of the paper:\n",
    "    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n",
    "    #   [...]\n",
    "    #   We leave the empirical investigation of [...], and biases to a future work.\n",
    "    \n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n",
    ")\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n",
      "Total number of parameters (original): 2,807,010\n",
      "Total number of parameters (original + LoRA): 2,813,804\n",
      "Parameters introduced by LoRA: 6,794\n",
      "Parameters incremment: 0.242%\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
    "    )\n",
    "# The non-LoRA parameters count must match the original network\n",
    "assert total_parameters_non_lora == total_parameters_original\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s, loss=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(mnist_trainset, batch_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m train(train_loader, net, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, total_iterations_limit\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[1;32m/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m loss_sum \u001b[39m/\u001b[39m num_iterations\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m data_iterator\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39mavg_loss)\n\u001b[0;32m---> <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m total_iterations_limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m total_iterations \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m total_iterations_limit:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load the MNIST dataset again, by keeping only the digit 9\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 9\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
    "train(train_loader, net, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
    "assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
    "assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n",
    "# The original weights have been moved to net.linear1.parametrizations.weight.original\n",
    "# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
    "assert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "# If we disable LoRA, the linear1.weight is the original one\n",
    "assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 567.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.101\n",
      "wrong counts for the digit 0: 980\n",
      "wrong counts for the digit 1: 1135\n",
      "wrong counts for the digit 2: 1032\n",
      "wrong counts for the digit 3: 1010\n",
      "wrong counts for the digit 4: 982\n",
      "wrong counts for the digit 5: 892\n",
      "wrong counts for the digit 6: 958\n",
      "wrong counts for the digit 7: 1028\n",
      "wrong counts for the digit 8: 974\n",
      "wrong counts for the digit 9: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 610.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.101\n",
      "wrong counts for the digit 0: 980\n",
      "wrong counts for the digit 1: 1135\n",
      "wrong counts for the digit 2: 1032\n",
      "wrong counts for the digit 3: 1010\n",
      "wrong counts for the digit 4: 982\n",
      "wrong counts for the digit 5: 892\n",
      "wrong counts for the digit 6: 958\n",
      "wrong counts for the digit 7: 1028\n",
      "wrong counts for the digit 8: 974\n",
      "wrong counts for the digit 9: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning on 9 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/595 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 412.13it/s, loss=0]\n"
     ]
    }
   ],
   "source": [
    "# for name, param in net.named_parameters():\n",
    "#     if 'lora' not in name:\n",
    "#         print(f'Freezing non-LoRA parameter {name}')\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# # Load the MNIST dataset again, by keeping only the digit 9\n",
    "# mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# exclude_indices = mnist_trainset.targets == 9\n",
    "# mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "# mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "# # Create a dataloader for the training\n",
    "# # include_indices = (mnist_trainset.targets == 9) | (mnist_trainset.targets == 3)\n",
    "\n",
    "# # Apply the mask to both data and targets\n",
    "# # mnist_trainset.data = mnist_trainset.data[include_indices]\n",
    "# # mnist_trainset.targets = mnist_trainset.targets[include_indices]\n",
    "# train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# # Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
    "# train(train_loader, net, epochs=1, total_iterations_limit=100)\n",
    "\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load the MNIST dataset again, by keeping only the digit 9\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 9\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
    "train(train_loader, net, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 626.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.915\n",
      "wrong counts for the digit 0: 57\n",
      "wrong counts for the digit 1: 17\n",
      "wrong counts for the digit 2: 126\n",
      "wrong counts for the digit 3: 22\n",
      "wrong counts for the digit 4: 164\n",
      "wrong counts for the digit 5: 79\n",
      "wrong counts for the digit 6: 33\n",
      "wrong counts for the digit 7: 169\n",
      "wrong counts for the digit 8: 157\n",
      "wrong counts for the digit 9: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try lora on VGG architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        out = self.layer9(out)\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        out = nn.AdaptiveAvgPool2d((1, 1))(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = VGG16(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb Cell 25\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer11(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer12(out)\n\u001b[0;32m---> <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer13(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m out \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mAdaptiveAvgPool2d((\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://10.127.30.133:10000/home/sarim.hashmi/Downloads/lora_implementation/lora_implementation.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    165\u001b[0m                         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, ceil_mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mceil_mode,\n\u001b[1;32m    166\u001b[0m                         return_indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_indices)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI702/lib/python3.12/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmax_pool2d(\u001b[39minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI702",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
