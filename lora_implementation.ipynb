{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the model is deterministic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor() ,\n",
    "\n",
    "transforms.Normalize((0.1307,),(0.3081,))\n",
    "\n",
    "])\n",
    "\n",
    "# mnist_trainset = datasets.MNIST(root='./',train=True , download=True , transform= transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(mnist_trainset , batch_size = 10 , shuffle = True)\n",
    "\n",
    "# mnist_testset = datasets.MNIST(root='../data' , train = False , download = True , transform = transform)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(mnist_testset,batch_size = 10 , shuffle = True)\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigboy(nn.Module):\n",
    "\n",
    "    def __init__(self , hidden_size_1 = 1000 , hidden_size_2 =2000):\n",
    "\n",
    "        super(bigboy,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28 , hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1 , hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2,10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self , img):\n",
    "\n",
    "        x = img.view(-1 , 28*28) # what does this do??\n",
    "\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net = bigboy().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:12<00:00, 493.20it/s, loss=0.24] \n"
     ]
    }
   ],
   "source": [
    "def train(train_loader , net , epochs=5 , total_iterations_limit = None):\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "        data_iterator = tqdm(train_loader,desc=f'Epoch {epoch+1}')\n",
    "\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "\n",
    "            num_iterations+=1\n",
    "            total_iterations+=1\n",
    "            x,y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1,28*28))\n",
    "\n",
    "            loss = criteria(output,y)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "\n",
    "                return\n",
    "\n",
    "train(train_loader,net,epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear1.weight': tensor([[ 0.0262,  0.0456, -0.0029,  ...,  0.0484,  0.0302,  0.0285],\n",
      "        [ 0.0123,  0.0171,  0.0216,  ...,  0.0118,  0.0261,  0.0021],\n",
      "        [ 0.0080,  0.0430, -0.0052,  ...,  0.0078,  0.0293,  0.0362],\n",
      "        ...,\n",
      "        [-0.0107,  0.0528,  0.0511,  ...,  0.0201,  0.0462, -0.0064],\n",
      "        [ 0.0733,  0.0366,  0.0223,  ...,  0.0566,  0.0502,  0.0510],\n",
      "        [ 0.0133, -0.0104,  0.0408,  ...,  0.0502,  0.0236,  0.0340]],\n",
      "       device='cuda:0'), 'linear1.bias': tensor([-5.1073e-02, -1.0745e-02, -3.5749e-02, -3.4667e-02, -5.9566e-04,\n",
      "        -4.2161e-02, -4.6397e-02, -3.9079e-02, -5.1368e-02, -7.2194e-03,\n",
      "        -2.4266e-02, -3.8512e-02, -2.7861e-02, -2.7927e-02,  1.2791e-02,\n",
      "        -2.0260e-02, -5.2526e-02, -2.6016e-02, -2.8698e-02, -5.0895e-02,\n",
      "        -3.2971e-02, -7.8956e-02,  2.7680e-02, -4.4517e-02, -2.7908e-02,\n",
      "        -4.5049e-02, -7.0258e-04, -2.2814e-02, -2.3985e-02, -1.6930e-02,\n",
      "         2.4631e-03, -2.3111e-02, -2.1943e-02, -6.7954e-03, -2.5725e-02,\n",
      "         1.4800e-02, -5.0559e-02,  2.5298e-03, -4.2388e-02, -4.8028e-03,\n",
      "        -7.3185e-03, -4.1934e-02,  8.8673e-03,  1.9569e-03, -5.0355e-03,\n",
      "         1.4365e-02, -5.8396e-02, -2.1542e-02, -3.5678e-02, -4.1494e-02,\n",
      "         6.9162e-03, -3.0659e-02, -1.3930e-02, -5.4586e-02, -3.9048e-02,\n",
      "        -1.7745e-02, -3.2438e-02, -8.9036e-02, -3.4331e-02, -1.2185e-02,\n",
      "        -5.0471e-02, -7.7672e-03,  1.0912e-02, -2.4253e-02, -4.8449e-02,\n",
      "        -7.0836e-02,  1.5485e-02, -3.2362e-02, -3.4783e-02, -5.4979e-02,\n",
      "        -4.4107e-02,  6.9615e-03, -4.6448e-02, -6.0564e-03, -1.3479e-02,\n",
      "        -1.4276e-02,  1.6145e-02, -2.3836e-02, -7.0902e-02,  5.0203e-03,\n",
      "         6.3875e-06, -4.7066e-02,  2.5213e-02, -1.5204e-02, -3.0745e-02,\n",
      "        -4.3543e-02, -4.1126e-02, -6.0163e-02,  9.8493e-03, -2.3570e-02,\n",
      "        -1.6852e-02, -3.3240e-02, -1.0813e-02,  2.0247e-02, -5.9178e-02,\n",
      "        -2.0973e-02, -3.8642e-02, -1.8092e-02, -8.7286e-03, -3.2485e-02,\n",
      "        -6.8829e-02,  2.3848e-02, -2.6982e-02,  2.1725e-03, -7.3132e-02,\n",
      "         1.7605e-02, -3.5636e-03,  4.3932e-03, -9.0012e-02, -4.0300e-02,\n",
      "        -1.3923e-02, -2.1941e-02, -7.2131e-02, -4.4919e-02, -6.7388e-02,\n",
      "         1.3885e-02, -1.0874e-02,  1.5994e-02, -3.6965e-02, -8.3121e-03,\n",
      "        -4.0030e-03, -6.1398e-02, -3.1127e-02, -1.3722e-02, -1.0768e-02,\n",
      "        -1.0367e-02, -9.5232e-03, -3.0132e-02, -1.1372e-02, -3.3183e-02,\n",
      "        -4.3359e-04,  3.0621e-02, -3.5163e-02,  1.6076e-02, -4.6491e-02,\n",
      "         2.1543e-03, -5.1139e-02, -9.7136e-03, -5.0621e-02,  3.8301e-03,\n",
      "        -4.2642e-02,  1.5194e-02, -2.5787e-02, -5.4094e-03, -7.5823e-02,\n",
      "        -4.9557e-03, -2.2902e-02, -6.9749e-03, -5.7982e-02, -7.4612e-02,\n",
      "        -1.1149e-03, -7.4217e-03, -4.0052e-02, -8.0322e-02, -4.1048e-02,\n",
      "        -5.2903e-03, -6.5911e-02, -2.8732e-02, -3.3973e-02, -1.7254e-02,\n",
      "        -1.6027e-03, -2.7293e-02,  1.9618e-02,  1.8275e-03, -3.4829e-02,\n",
      "        -2.8004e-02, -2.5290e-02, -3.8401e-02, -4.2799e-02, -1.2802e-02,\n",
      "        -4.5913e-02, -5.7349e-02, -3.3481e-02,  1.9260e-02, -2.6796e-02,\n",
      "        -4.2291e-02,  4.8824e-03, -2.4360e-02, -3.6757e-03, -3.9532e-02,\n",
      "         9.4501e-03, -6.5769e-02, -5.4256e-02, -4.3276e-02,  5.5821e-04,\n",
      "        -6.2793e-02,  1.0483e-02, -4.1290e-02, -9.3556e-03, -2.0943e-02,\n",
      "        -4.4789e-02,  4.3667e-03, -5.6722e-02, -4.5242e-02,  1.1689e-03,\n",
      "        -2.1800e-02, -2.8726e-03, -3.4861e-02, -3.7792e-02, -7.6626e-02,\n",
      "         6.3769e-03, -1.2701e-02, -4.4584e-02, -2.8413e-02, -1.3008e-02,\n",
      "         6.5791e-03, -4.3162e-02,  1.7892e-02, -3.9958e-02, -2.4304e-02,\n",
      "        -8.4659e-04, -3.7090e-02, -2.1933e-02, -1.0113e-02, -1.7396e-05,\n",
      "        -4.6019e-02, -2.3862e-02, -1.2718e-02,  6.1236e-03, -2.3623e-02,\n",
      "        -6.3935e-02, -1.2010e-02,  1.4748e-02, -7.4502e-03, -3.7235e-02,\n",
      "        -5.9089e-03, -2.9836e-02, -4.8492e-03,  6.9443e-03, -2.9006e-02,\n",
      "        -6.2860e-02, -3.7189e-02, -3.3821e-03, -5.0711e-02, -3.9658e-02,\n",
      "        -7.2323e-03, -7.4543e-02,  1.1078e-02, -3.6935e-02,  1.8196e-03,\n",
      "        -2.8372e-02, -3.0399e-02, -3.8863e-02, -4.4454e-02, -3.9294e-02,\n",
      "        -3.9742e-02, -2.3191e-02, -2.1454e-02, -4.8568e-02, -1.8493e-02,\n",
      "        -2.0801e-02, -4.2839e-02, -3.4890e-03, -6.3387e-02,  3.3350e-03,\n",
      "        -2.9186e-02, -2.8952e-02, -1.8833e-02, -4.4099e-02, -8.3291e-02,\n",
      "        -5.1020e-02, -4.7167e-03, -6.5138e-02, -5.7762e-02, -1.1836e-02,\n",
      "        -1.5662e-02, -1.3457e-02, -1.1076e-02, -1.3911e-02,  7.5274e-04,\n",
      "        -4.5650e-02,  2.0803e-02, -7.7114e-02, -2.7996e-02, -1.2129e-02,\n",
      "        -3.9871e-02,  6.2093e-03, -2.1231e-02, -7.1780e-02,  6.6083e-03,\n",
      "        -2.0697e-02, -1.9370e-02, -1.5226e-02, -1.4271e-02, -2.0926e-03,\n",
      "        -4.0015e-02, -5.4734e-02, -8.1332e-03,  7.8629e-03, -2.4827e-02,\n",
      "        -6.3269e-02, -5.2328e-02, -8.9947e-04,  3.4740e-04, -3.3430e-02,\n",
      "        -2.4128e-02,  9.2285e-04, -2.8359e-03, -6.0983e-02, -3.4883e-04,\n",
      "        -1.1359e-02, -2.5761e-02, -5.8365e-02, -1.4834e-02, -8.1081e-02,\n",
      "         4.1972e-03, -2.8968e-02, -1.4427e-02, -2.3957e-02, -4.6820e-03,\n",
      "        -3.3465e-02,  1.1626e-02, -1.1641e-02, -7.2030e-02,  2.5745e-02,\n",
      "        -8.7622e-03, -4.5568e-02,  1.0384e-02,  5.5651e-03, -3.1304e-02,\n",
      "        -2.3218e-04, -3.5781e-02, -7.5496e-03, -1.6670e-02, -6.3489e-02,\n",
      "         4.7063e-03,  7.3405e-03, -4.2442e-02, -4.9610e-02, -3.3376e-02,\n",
      "        -8.6278e-03, -3.7529e-03, -1.0357e-02, -2.2847e-02, -5.7676e-02,\n",
      "        -3.4961e-02, -3.8720e-02, -3.9122e-02,  1.2549e-02, -2.0433e-02,\n",
      "         1.5371e-02, -5.0435e-02, -5.9501e-02, -6.1779e-02, -3.3050e-02,\n",
      "        -2.1383e-04, -5.0863e-02, -4.2865e-02, -1.6555e-02, -7.3867e-03,\n",
      "        -3.0086e-02, -2.1752e-02, -4.5863e-02, -3.2723e-02, -6.0702e-02,\n",
      "        -3.1403e-03, -2.7353e-02, -4.0722e-02,  9.2871e-03,  2.9149e-03,\n",
      "         1.3412e-02, -1.2088e-02, -2.6205e-02, -1.3831e-02, -6.8675e-03,\n",
      "         2.5803e-03, -3.3884e-02, -4.4063e-03, -2.4605e-03,  2.6775e-02,\n",
      "        -2.9659e-02,  3.1516e-03,  2.5975e-02, -1.6097e-02, -1.2442e-02,\n",
      "        -5.3176e-02,  9.7039e-03, -3.2512e-02,  1.2480e-02, -4.4080e-02,\n",
      "        -3.8968e-02, -1.3497e-02, -7.0415e-02,  1.9799e-02, -4.2927e-02,\n",
      "        -4.6297e-02, -4.7552e-03,  2.3468e-03, -3.6899e-02, -1.5151e-03,\n",
      "        -3.1691e-02, -1.8825e-02, -1.0984e-02,  9.9094e-03, -1.6095e-02,\n",
      "        -3.8002e-02, -5.8385e-02, -4.8645e-03,  2.0804e-02, -1.4830e-02,\n",
      "        -5.1660e-02,  1.0253e-03, -4.7956e-02, -1.1842e-02, -8.0815e-02,\n",
      "         1.4395e-02, -1.4725e-02,  7.1297e-03,  2.0518e-03, -4.7760e-02,\n",
      "        -2.5739e-02, -1.3222e-02, -3.1136e-02, -2.2020e-02, -3.0676e-02,\n",
      "        -3.0427e-02, -3.7870e-02, -1.1207e-02, -3.8111e-02,  3.9573e-03,\n",
      "        -6.2676e-02, -3.2668e-02, -7.1681e-02, -1.2550e-02, -5.5165e-03,\n",
      "        -8.4666e-02,  1.8245e-02,  2.5942e-02, -9.4211e-03, -1.1824e-02,\n",
      "        -1.2177e-02, -4.2431e-02,  1.8775e-02, -2.7401e-02, -6.1831e-02,\n",
      "        -3.0864e-02, -4.1369e-02, -4.2213e-02, -8.3871e-02, -1.0057e-03,\n",
      "         1.4779e-02, -9.9909e-03, -7.5050e-02, -2.0965e-02, -5.8171e-02,\n",
      "         1.3398e-02, -3.6032e-02, -6.5374e-02,  3.3417e-04, -7.0620e-03,\n",
      "        -3.4179e-02, -2.3404e-02,  1.7308e-02, -4.8838e-02, -4.6440e-02,\n",
      "        -1.7903e-02, -1.7604e-03, -3.8489e-02, -2.2310e-02, -4.1228e-02,\n",
      "        -3.6433e-02, -1.6027e-02, -1.0736e-04, -5.8979e-02,  4.1830e-03,\n",
      "        -5.1202e-02, -6.0140e-02, -2.2711e-02, -8.0188e-03, -2.6989e-02,\n",
      "         1.0767e-02, -2.3491e-02, -2.7091e-02, -4.0116e-02, -4.6466e-02,\n",
      "        -4.0475e-02, -2.4183e-02,  9.7334e-03, -2.2939e-02, -1.7071e-02,\n",
      "        -3.3911e-02, -4.3000e-02, -3.5987e-02, -1.0196e-02, -6.8998e-03,\n",
      "        -5.8738e-03, -4.6175e-03, -4.2025e-02, -5.9195e-02, -4.5395e-02,\n",
      "        -9.3901e-02, -2.0405e-02, -3.7824e-02,  7.0404e-03,  6.3956e-03,\n",
      "        -4.2549e-02, -4.8164e-02, -2.1191e-02, -3.6897e-02, -6.1369e-02,\n",
      "        -3.7142e-03, -8.4062e-03, -5.2607e-02, -1.8024e-02, -1.4476e-03,\n",
      "        -1.1832e-02, -6.3380e-02, -3.9470e-02, -1.9521e-02, -4.6727e-02,\n",
      "        -4.6289e-02,  6.2253e-03, -5.6948e-03, -4.2665e-02, -4.8023e-02,\n",
      "         1.4121e-02, -3.5080e-03, -6.2739e-03, -4.1234e-02, -1.0873e-02,\n",
      "         1.9119e-02, -6.5654e-02, -4.2671e-02, -2.2111e-02, -4.2345e-02,\n",
      "        -6.9844e-02, -1.0082e-02, -4.3252e-02,  6.3459e-03, -5.8606e-02,\n",
      "        -5.3523e-02, -2.2081e-02, -9.1484e-02,  2.8490e-02,  1.1567e-02,\n",
      "        -3.9347e-02, -2.9545e-02, -5.2233e-02, -3.2326e-02, -5.1828e-02,\n",
      "        -5.3747e-02, -4.4617e-03, -3.1770e-02, -2.1973e-02, -4.6542e-02,\n",
      "        -5.4722e-02, -4.8875e-02, -3.5187e-02,  1.7719e-02,  1.0999e-02,\n",
      "        -2.9912e-02, -2.1380e-02, -3.1236e-02, -3.1681e-02,  1.9273e-02,\n",
      "         3.5378e-03, -4.4020e-02, -4.7309e-02, -5.1000e-02, -3.6260e-02,\n",
      "        -5.5965e-03, -4.6268e-02, -5.6277e-02, -4.8881e-03,  4.2185e-03,\n",
      "        -3.9955e-02, -4.3459e-02, -2.3495e-02, -2.4793e-02,  7.1177e-03,\n",
      "        -5.6266e-02, -2.1126e-02, -5.2525e-02, -1.7517e-02, -5.6832e-02,\n",
      "        -1.4350e-02, -5.7741e-02,  3.1173e-04, -4.6179e-02, -6.8236e-02,\n",
      "        -5.1261e-02, -4.3076e-02,  5.9818e-06, -1.6865e-02, -7.1471e-02,\n",
      "        -6.3113e-02, -4.8632e-02, -3.0117e-02, -9.0907e-03,  1.9357e-02,\n",
      "        -3.0380e-02, -3.3796e-02, -8.4452e-02,  3.5706e-03, -5.5558e-02,\n",
      "        -3.8960e-02, -5.6570e-02, -6.1399e-02, -3.8828e-02, -2.8861e-02,\n",
      "        -1.9222e-02,  1.7737e-02, -2.6774e-02,  3.8365e-03, -3.4111e-02,\n",
      "         3.7898e-03, -3.4011e-02,  1.2524e-02,  3.3624e-03, -2.9428e-02,\n",
      "        -4.0193e-02, -5.2980e-02, -1.0366e-02, -2.8362e-02, -4.1701e-02,\n",
      "        -1.9387e-02, -8.7493e-02, -2.4901e-02, -1.8242e-02, -5.7833e-02,\n",
      "        -2.1088e-02, -4.0563e-02, -4.9975e-02, -6.3228e-03, -4.2640e-02,\n",
      "        -2.2034e-02, -2.0516e-02,  1.4474e-03, -5.7290e-02,  5.9950e-03,\n",
      "        -3.3835e-02, -1.4206e-02,  2.3631e-02, -4.9231e-02, -2.3170e-02,\n",
      "        -4.4980e-02,  4.6799e-03, -4.5135e-02, -2.4693e-03, -2.3311e-02,\n",
      "        -1.1974e-02, -3.9941e-02,  1.0856e-02, -5.9434e-02, -2.2365e-02,\n",
      "         8.5562e-04, -7.5466e-02, -4.4872e-02, -2.8012e-02, -3.9178e-02,\n",
      "        -4.6445e-02, -5.3146e-02, -5.6145e-02,  1.4389e-02,  4.2009e-03,\n",
      "        -8.3590e-02, -4.7482e-02, -6.9175e-02, -5.7412e-04,  2.4758e-02,\n",
      "        -3.4855e-02, -4.6607e-02, -5.7273e-02, -2.3554e-02, -6.8855e-02,\n",
      "        -2.5057e-03, -1.8361e-02, -5.8140e-02, -7.4095e-03, -3.1871e-02,\n",
      "        -4.0301e-02, -1.2616e-02, -6.4822e-02, -7.5302e-02, -8.4880e-05,\n",
      "        -1.2619e-02, -1.2651e-02, -2.1677e-02, -1.8576e-02, -6.1922e-02,\n",
      "        -3.8813e-02, -2.3032e-02, -2.9397e-02, -3.3878e-02, -4.4910e-02,\n",
      "        -5.4289e-04, -3.0620e-02, -1.5738e-02, -3.4514e-02, -4.1644e-02,\n",
      "        -5.2819e-02, -5.9007e-02, -4.2541e-02, -1.0623e-02, -2.3060e-02,\n",
      "        -4.6976e-02, -6.1939e-03, -2.9242e-03, -4.0429e-02, -3.2636e-02,\n",
      "        -3.2865e-02, -1.0498e-02, -3.4860e-02, -4.7487e-02, -3.0645e-02,\n",
      "         4.7056e-03, -8.4700e-03, -5.1929e-02,  1.6930e-02, -1.1951e-02,\n",
      "        -6.7726e-02, -6.4212e-02,  3.0455e-03, -6.7619e-03, -1.4259e-02,\n",
      "        -1.6137e-02, -8.8351e-02,  1.8649e-02, -8.7713e-03, -7.1661e-02,\n",
      "        -4.8181e-02, -4.5777e-02, -4.0182e-02, -3.8571e-02, -1.8518e-02,\n",
      "        -5.8800e-02, -3.8295e-02, -2.8500e-02, -5.9529e-02, -8.5934e-03,\n",
      "        -2.9395e-02, -1.6108e-02, -6.0181e-02, -3.3676e-02, -3.3471e-02,\n",
      "        -5.8775e-02, -1.4203e-02, -4.2407e-02, -4.2837e-02, -2.6707e-02,\n",
      "        -6.6640e-03,  9.3641e-03, -4.2013e-02, -2.6964e-02,  1.8250e-02,\n",
      "         3.8924e-03, -4.5820e-02, -2.1455e-02, -2.7919e-02,  1.4458e-02,\n",
      "         6.4885e-03, -4.9764e-02, -3.8872e-02, -1.6744e-02,  1.8443e-02,\n",
      "        -1.0164e-02, -3.1644e-02, -1.6029e-02, -7.7485e-02, -5.0995e-03,\n",
      "        -4.3884e-02, -1.1255e-02, -2.4142e-02,  2.5414e-03,  2.2186e-02,\n",
      "        -4.8047e-02, -4.4425e-02, -4.5981e-02, -3.6558e-02, -6.8140e-02,\n",
      "        -1.9178e-03, -2.4671e-02, -1.3404e-02, -5.0871e-02,  9.0095e-03,\n",
      "        -4.7261e-02, -8.4249e-02, -3.9431e-02, -5.6528e-02,  2.5971e-03,\n",
      "        -2.8249e-02,  1.4009e-02, -3.0619e-06, -4.9218e-03, -6.1202e-02,\n",
      "         1.1641e-02, -7.8227e-03, -1.8564e-02, -1.9862e-02, -3.3054e-02,\n",
      "        -1.6995e-02, -9.3108e-02, -3.0728e-05, -3.4236e-02, -2.6999e-02,\n",
      "        -5.0590e-02, -3.8709e-02,  1.7630e-03, -3.8116e-03, -1.5894e-02,\n",
      "        -5.1293e-02, -1.9407e-02, -5.6254e-02,  1.0437e-02,  1.1259e-02,\n",
      "         2.7969e-03, -6.3990e-03, -6.3843e-02,  5.3248e-03,  1.8366e-02,\n",
      "         1.0576e-03, -2.9416e-02,  1.2541e-02, -3.3365e-03, -2.5477e-02,\n",
      "         1.7125e-02, -2.3362e-02, -3.1455e-02, -1.1096e-02, -4.3506e-02,\n",
      "         1.4705e-03, -4.8668e-02, -3.2898e-02, -6.8118e-02,  6.5668e-03,\n",
      "        -4.7478e-02, -3.7438e-02, -7.3263e-02, -1.6567e-02, -2.3594e-02,\n",
      "        -5.2954e-02, -1.2860e-02, -1.5118e-02, -2.5656e-02, -3.3605e-02,\n",
      "        -1.9433e-02, -2.7302e-02, -5.7599e-02, -2.0214e-02, -4.1266e-02,\n",
      "         2.3323e-02, -8.1116e-02, -2.7799e-02, -1.3512e-02, -6.3033e-02,\n",
      "        -2.9405e-02, -6.0289e-02, -7.4278e-03, -5.0484e-02,  1.8355e-02,\n",
      "        -8.7909e-02, -9.5339e-03, -1.3856e-02, -2.8784e-02, -6.4682e-03,\n",
      "        -6.7584e-03, -3.3055e-02, -3.5032e-02, -1.0388e-02,  5.2521e-03,\n",
      "        -8.4892e-04, -2.3116e-02, -3.6040e-02, -4.3762e-03, -3.9379e-02,\n",
      "        -5.1729e-02, -5.3703e-02, -3.7204e-02,  1.1571e-02, -1.5712e-02,\n",
      "        -1.6506e-02,  5.0252e-03, -7.0919e-02, -5.6407e-02, -1.1147e-01,\n",
      "        -4.8765e-02,  8.8291e-03, -1.4238e-02, -1.3637e-02, -2.6812e-02,\n",
      "        -1.2652e-02, -3.1196e-02, -3.1848e-02,  2.2711e-02, -6.5525e-02,\n",
      "        -3.2041e-02, -3.8627e-02, -7.0755e-02, -3.0110e-02,  5.8827e-03,\n",
      "        -6.2961e-03,  1.0464e-02, -2.6153e-02, -3.3276e-02, -8.4802e-03,\n",
      "         1.9748e-03, -1.0432e-02,  3.5874e-02, -9.3197e-02, -1.4505e-02,\n",
      "        -4.0262e-02,  4.4382e-04, -4.1000e-02, -2.7579e-02, -3.7966e-02,\n",
      "        -3.8156e-02, -1.2435e-02,  2.2652e-03, -6.8276e-02,  1.7741e-02,\n",
      "        -5.5807e-02, -7.3391e-02,  5.5426e-03, -2.2539e-02, -4.8992e-02,\n",
      "         4.9977e-03, -4.0915e-02, -5.4846e-02, -4.0363e-02, -3.3283e-02,\n",
      "        -1.9542e-03, -1.8628e-02,  2.6061e-02, -2.4482e-02, -5.3228e-02,\n",
      "        -4.6228e-02, -1.9755e-02, -5.0449e-02, -2.8868e-02, -5.0502e-02,\n",
      "        -2.0840e-02, -3.0989e-02, -1.5749e-02, -9.3775e-02, -3.3436e-02,\n",
      "        -5.2629e-02,  1.4220e-02, -2.9267e-02, -3.3923e-02, -5.3734e-02,\n",
      "        -7.2432e-02, -1.6323e-02, -3.0108e-02, -5.7729e-02, -3.9866e-02,\n",
      "        -5.3001e-02, -3.9006e-02, -4.5158e-02, -6.9197e-03, -1.5263e-02,\n",
      "        -4.6537e-02, -1.0416e-02, -1.0589e-02, -1.2122e-02, -4.5271e-02,\n",
      "        -3.0903e-02, -5.8353e-02, -3.3534e-02, -6.6956e-03, -7.7351e-02,\n",
      "        -6.1595e-02, -1.8594e-02, -2.7335e-02, -6.4551e-02, -2.2500e-02,\n",
      "        -7.6953e-03,  1.7922e-02, -3.3847e-02, -6.5298e-02, -3.1935e-02,\n",
      "        -4.3197e-02, -2.0197e-02, -4.2207e-02, -2.9201e-02,  1.4115e-02,\n",
      "        -3.3550e-02, -4.3066e-02, -5.2146e-02, -6.3805e-02, -8.9391e-03,\n",
      "        -1.6466e-02, -4.7883e-02, -2.9941e-02, -2.6902e-02, -3.5198e-02,\n",
      "        -4.3065e-02, -3.1558e-02, -6.5589e-02, -2.6587e-03, -2.8456e-02,\n",
      "        -1.1877e-02, -1.2551e-02, -5.6053e-02, -1.0308e-02, -6.6680e-02,\n",
      "        -2.7994e-02, -4.2190e-02,  7.1891e-04, -4.7520e-02, -1.4148e-02],\n",
      "       device='cuda:0'), 'linear2.weight': tensor([[-0.0745,  0.0304,  0.1662,  ..., -0.0171, -0.0346,  0.0743],\n",
      "        [-0.0434,  0.0819, -0.0738,  ...,  0.0188,  0.0268, -0.0816],\n",
      "        [-0.0929,  0.0127, -0.1000,  ...,  0.0056,  0.0119, -0.0357],\n",
      "        ...,\n",
      "        [ 0.0121, -0.0100, -0.0015,  ..., -0.0396, -0.0389,  0.0032],\n",
      "        [-0.0289, -0.0093, -0.0135,  ..., -0.0190, -0.0357, -0.0032],\n",
      "        [-0.0152,  0.0161, -0.0334,  ..., -0.0386, -0.0314, -0.0201]],\n",
      "       device='cuda:0'), 'linear2.bias': tensor([-0.0221, -0.1326, -0.0568,  ..., -0.0688, -0.0497, -0.0891],\n",
      "       device='cuda:0'), 'linear3.weight': tensor([[-0.0350,  0.0507, -0.0074,  ...,  0.0023, -0.0297,  0.0026],\n",
      "        [ 0.0097, -0.0193, -0.0495,  ..., -0.0257,  0.0126, -0.0056],\n",
      "        [ 0.0271, -0.0167, -0.0270,  ..., -0.0162, -0.0283, -0.0095],\n",
      "        ...,\n",
      "        [-0.0439, -0.0023,  0.0269,  ..., -0.0020,  0.0031, -0.0816],\n",
      "        [ 0.0157, -0.0025, -0.0144,  ..., -0.0353,  0.0023, -0.0316],\n",
      "        [-0.0429, -0.0218, -0.0666,  ..., -0.0601, -0.0319, -0.0087]],\n",
      "       device='cuda:0'), 'linear3.bias': tensor([ 0.0125, -0.0401, -0.0438, -0.0425, -0.0299, -0.0442, -0.0385, -0.0514,\n",
      "         0.2046,  0.0427], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "original_weights = {}\n",
    "\n",
    "for name , param in net.named_parameters():\n",
    "\n",
    "    original_weights[name] = param.clone().detach()\n",
    "\n",
    "print(original_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 685.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956\n",
      "wrong counts for the digit 0: 16\n",
      "wrong counts for the digit 1: 19\n",
      "wrong counts for the digit 2: 55\n",
      "wrong counts for the digit 3: 112\n",
      "wrong counts for the digit 4: 20\n",
      "wrong counts for the digit 5: 23\n",
      "wrong counts for the digit 6: 36\n",
      "wrong counts for the digit 7: 47\n",
      "wrong counts for the digit 8: 18\n",
      "wrong counts for the digit 9: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in tqdm(test_loader , desc='Testing'):\n",
    "\n",
    "                x,y = data\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                output = net(x.view(-1,784))\n",
    "\n",
    "                for idx , i in enumerate(output):\n",
    "\n",
    "                    if torch.argmax(i) == y[idx]:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        wrong_counts[y[idx]] +=1\n",
    "                    total+=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets calculate the total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
      "Total number of parameters: 2,807,010\n"
     ]
    }
   ],
   "source": [
    "total_parameters_original = 0\n",
    "\n",
    "for index , layer in enumerate([net.linear1 , net.linear2 , net.linear3]):\n",
    "\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "\n",
    "print(f'Total number of parameters: {total_parameters_original:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets define LoRA parameterization : https://pytorch.org/tutorials/intermediate/parametrizations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "        super().__init__()\n",
    "        # Section 4.1 of the paper: \n",
    "        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "        \n",
    "        # Section 4.1 of the paper: \n",
    "        #   We then scale ∆Wx by α/r , where α is a constant in r. \n",
    "        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n",
    "        #   As a result, we simply set α to the first r we try and do not tune it. \n",
    "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # Return W + (B*A)*scale\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
    "\n",
    "    # From section 4.2 of the paper:\n",
    "    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n",
    "    #   [...]\n",
    "    #   We leave the empirical investigation of [...], and biases to a future work.\n",
    "    \n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n",
    ")\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI702",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
