{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [6000/6000], Loss: 0.0976\n",
      "Accuracy of the network on the 5000 validation images: 97.75 %\n",
      "Epoch [2/10], Step [6000/6000], Loss: 0.0016\n",
      "Accuracy of the network on the 5000 validation images: 98.0 %\n",
      "Epoch [3/10], Step [6000/6000], Loss: 0.0080\n",
      "Accuracy of the network on the 5000 validation images: 97.55 %\n",
      "Epoch [4/10], Step [6000/6000], Loss: 0.0281\n",
      "Accuracy of the network on the 5000 validation images: 97.56 %\n",
      "Epoch [5/10], Step [6000/6000], Loss: 0.0004\n",
      "Accuracy of the network on the 5000 validation images: 98.61 %\n",
      "Epoch [6/10], Step [6000/6000], Loss: 0.0869\n",
      "Accuracy of the network on the 5000 validation images: 98.19 %\n",
      "Epoch [7/10], Step [6000/6000], Loss: 0.0249\n",
      "Accuracy of the network on the 5000 validation images: 97.71 %\n",
      "Epoch [8/10], Step [6000/6000], Loss: 0.0294\n",
      "Accuracy of the network on the 5000 validation images: 98.69 %\n",
      "Epoch [9/10], Step [6000/6000], Loss: 0.2552\n",
      "Accuracy of the network on the 5000 validation images: 96.76 %\n",
      "Epoch [10/10], Step [6000/6000], Loss: 0.0213\n",
      "Accuracy of the network on the 5000 validation images: 98.59 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "_ = torch.manual_seed(0)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor() ,\n",
    "\n",
    "transforms.Normalize((0.1307,),(0.3081,))\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SimplifiedVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimplifiedVGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 3 * 3, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SimplifiedVGG16(num_classes=10).to(device)\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.005\n",
    "\n",
    "model =SimplifiedVGG16(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {}\n",
    "\n",
    "for name , param in model.named_parameters():\n",
    "\n",
    "    original_weights[name] = param.clone().detach()\n",
    "\n",
    "print(original_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now test this model on each numbers in mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 579.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.986\n",
      "wrong counts for the digit 0: 7\n",
      "wrong counts for the digit 1: 10\n",
      "wrong counts for the digit 2: 7\n",
      "wrong counts for the digit 3: 5\n",
      "wrong counts for the digit 4: 31\n",
      "wrong counts for the digit 5: 11\n",
      "wrong counts for the digit 6: 14\n",
      "wrong counts for the digit 7: 30\n",
      "wrong counts for the digit 8: 13\n",
      "wrong counts for the digit 9: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in tqdm(test_loader , desc='Testing'):\n",
    "\n",
    "                x,y = data\n",
    "\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                output = model(x)\n",
    "\n",
    "                for idx , i in enumerate(output):\n",
    "\n",
    "                    if torch.argmax(i) == y[idx]:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        wrong_counts[y[idx]] +=1\n",
    "                    total+=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Total number of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer features.0: W: torch.Size([64, 1, 3, 3]) + B: torch.Size([64])\n",
      "Parameters: 640\n",
      "Layer features.3: W: torch.Size([128, 64, 3, 3]) + B: torch.Size([128])\n",
      "Parameters: 73856\n",
      "Layer features.6: W: torch.Size([256, 128, 3, 3]) + B: torch.Size([256])\n",
      "Parameters: 295168\n",
      "Layer features.8: W: torch.Size([256, 256, 3, 3]) + B: torch.Size([256])\n",
      "Parameters: 590080\n",
      "Layer classifier.0: W: torch.Size([1024, 2304]) + B: torch.Size([1024])\n",
      "Parameters: 2360320\n",
      "Layer classifier.2: W: torch.Size([10, 1024]) + B: torch.Size([10])\n",
      "Parameters: 10250\n",
      "Total number of parameters: 3,330,314\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        weights_params = module.weight.nelement()\n",
    "        bias_params = module.bias.nelement() if module.bias is not None else 0\n",
    "        total_params += weights_params + bias_params\n",
    "        print(f'Layer {name}: W: {module.weight.shape} + B: {module.bias.shape if module.bias is not None else \"No bias\"}')\n",
    "        print(f'Parameters: {weights_params + bias_params}')\n",
    "\n",
    "print(f'Total number of parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/sarim.hashmi/anaconda3/envs/AI702/lib/python3.12/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]             640\n",
      "              ReLU-2           [-1, 64, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
      "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
      "              ReLU-5          [-1, 128, 14, 14]               0\n",
      "         MaxPool2d-6            [-1, 128, 7, 7]               0\n",
      "            Conv2d-7            [-1, 256, 7, 7]         295,168\n",
      "              ReLU-8            [-1, 256, 7, 7]               0\n",
      "            Conv2d-9            [-1, 256, 7, 7]         590,080\n",
      "             ReLU-10            [-1, 256, 7, 7]               0\n",
      "        MaxPool2d-11            [-1, 256, 3, 3]               0\n",
      "           Linear-12                 [-1, 1024]       2,360,320\n",
      "             ReLU-13                 [-1, 1024]               0\n",
      "           Linear-14                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 3,330,314\n",
      "Trainable params: 3,330,314\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.71\n",
      "Params size (MB): 12.70\n",
      "Estimated Total Size (MB): 14.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "        super().__init__()\n",
    "        # Section 4.1 of the paper: \n",
    "        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "        \n",
    "        # Section 4.1 of the paper: \n",
    "        #   We then scale ∆Wx by α/r , where α is a constant in r. \n",
    "        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n",
    "        #   As a result, we simply set α to the first r we try and do not tune it. \n",
    "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # Return W + (B*A)*scale\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def conv_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    features_in = layer.weight.shape[1] * layer.weight.shape[2] * layer.weight.shape[3]\n",
    "    features_out = layer.weight.shape[0]\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "def add_lora_to_model(model, device, rank=1, lora_alpha=1):\n",
    "    # Add LoRA to convolutional layers\n",
    "    for i, layer in enumerate(model.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            parametrize.register_parametrization(\n",
    "                layer, \"weight\", conv_layer_parameterization(layer, device, rank, lora_alpha)\n",
    "            )\n",
    "\n",
    "    # Add LoRA to linear layers\n",
    "    for i, layer in enumerate(model.classifier):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            parametrize.register_parametrization(\n",
    "                layer, \"weight\", linear_layer_parameterization(layer, device, rank, lora_alpha)\n",
    "            )\n",
    "\n",
    "def enable_disable_lora(model, enabled=True):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)) and hasattr(module, 'parametrizations'):\n",
    "            module.parametrizations[\"weight\"][0].enabled = enabled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params_original = 0\n",
    "    total_params_lora = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            weights_params = module.weight.nelement()\n",
    "            bias_params = module.bias.nelement() if module.bias is not None else 0\n",
    "            total_params_original += weights_params + bias_params\n",
    "            \n",
    "            if hasattr(module, 'parametrizations'):\n",
    "                lora_params = module.parametrizations[\"weight\"][0].lora_A.nelement() + \\\n",
    "                              module.parametrizations[\"weight\"][0].lora_B.nelement()\n",
    "                total_params_lora += lora_params\n",
    "                \n",
    "                print(f'Layer {name}:')\n",
    "                print(f'  W: {module.weight.shape}')\n",
    "                print(f'  B: {module.bias.shape if module.bias is not None else \"No bias\"}')\n",
    "                print(f'  Lora_A: {module.parametrizations[\"weight\"][0].lora_A.shape}')\n",
    "                print(f'  Lora_B: {module.parametrizations[\"weight\"][0].lora_B.shape}')\n",
    "                print(f'  Parameters: {weights_params + bias_params + lora_params}')\n",
    "            else:\n",
    "                print(f'Layer {name}:')\n",
    "                print(f'  W: {module.weight.shape}')\n",
    "                print(f'  B: {module.bias.shape if module.bias is not None else \"No bias\"}')\n",
    "                print(f'  Parameters: {weights_params + bias_params}')\n",
    "    \n",
    "    print(f'\\nTotal number of parameters (original): {total_params_original:,}')\n",
    "    print(f'Total number of parameters (original + LoRA): {total_params_original + total_params_lora:,}')\n",
    "    print(f'Parameters introduced by LoRA: {total_params_lora:,}')\n",
    "    parameters_increment = (total_params_lora / total_params_original) * 100\n",
    "    print(f'Parameters increment: {parameters_increment:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters before adding LoRA:\")\n",
    "count_parameters(model)\n",
    "\n",
    "# Add LoRA to the model\n",
    "add_lora_to_model(model, device, rank=4, lora_alpha=1)\n",
    "\n",
    "# Count parameters after adding LoRA\n",
    "print(\"\\nParameters after adding LoRA:\")\n",
    "count_parameters(model)\n",
    "\n",
    "# Example of enabling/disabling LoRA\n",
    "enable_disable_lora(model, enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets finetune for 9  , 4 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader , model , epochs=5 , total_iterations_limit = None):\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "        # Train the model\n",
    "        total_step = len(train_loader)\n",
    "\n",
    "        total_step = len(train_loader)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):  \n",
    "                # Move tensors to the configured device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "                    \n",
    "            # Validation\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    del images, labels, outputs\n",
    "            \n",
    "                print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if 'lora' not in name:\n",
    "#         print(f'Freezing non-LoRA parameter {name}')\n",
    "#         param.requires_grad = False\n",
    "\n",
    "\n",
    "# mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# exclude_indices = mnist_trainset.targets == 9\n",
    "# mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "# mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# train(train_loader, model, epochs=1, total_iterations_limit=100)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load the MNIST dataset again, by keeping only the digit 9\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a dataloader for the training\n",
    "include_indices = (mnist_trainset.targets == 9) | (mnist_trainset.targets == 4) | (mnist_trainset.targets == 5)\n",
    "\n",
    "# Apply the mask to both data and targets\n",
    "mnist_trainset.data = mnist_trainset.data[include_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[include_indices]\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
    "train(train_loader, model, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:02<00:00, 483.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972\n",
      "wrong counts for the digit 0: 23\n",
      "wrong counts for the digit 1: 9\n",
      "wrong counts for the digit 2: 23\n",
      "wrong counts for the digit 3: 32\n",
      "wrong counts for the digit 4: 8\n",
      "wrong counts for the digit 5: 3\n",
      "wrong counts for the digit 6: 41\n",
      "wrong counts for the digit 7: 87\n",
      "wrong counts for the digit 8: 43\n",
      "wrong counts for the digit 9: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(model,enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:01<00:00, 558.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.986\n",
      "wrong counts for the digit 0: 7\n",
      "wrong counts for the digit 1: 10\n",
      "wrong counts for the digit 2: 7\n",
      "wrong counts for the digit 3: 5\n",
      "wrong counts for the digit 4: 31\n",
      "wrong counts for the digit 5: 11\n",
      "wrong counts for the digit 6: 14\n",
      "wrong counts for the digit 7: 30\n",
      "wrong counts for the digit 8: 13\n",
      "wrong counts for the digit 9: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(model,enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI702",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
